---
title: "Self-Conscious Emotions in Sport and Exercise: Relationships of Implicit and Explicit Processes of Authentic and Hubristic Pride and Physical Activity"
shorttitle: "Self-Conscious Emotions in Sport"
author:
  - name: Sascha Leisterer
    corresponding: true
    orcid: 0000-0000-0000-0001
    email: sascha.leisterer@uni-leipzig.de
    affiliations:
      - name: Leipzig University
        department: Department of Sport Science
        address: Jahnallee 59
        city: Leipzig
        region: Saxony
        country: Germany
        postal-code:
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: ~
    related-report: ~
    conflict-of-interest: ~
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
link-citations: true
abstract: "Self-conscious emotions play a crucial role in sports and exercise by reflecting self-related goal achievements. Traditionally, these emotions are explained through explicit attributional processes. However, implicit processes that influence self-related goal orientation are often overlooked."
keywords: [keyword1, keyword2, keyword3]
bibliography: bibliography.bib
format:
  apaquarto-html: default
  apaquarto-docx: default
  #apaquarto-pdf:
    #documentmode: man
floatsintext: false
comments:
  hypothesis: true
---

```{r setup}
#| include: false

if (!requireNamespace("needs", quietly = TRUE)) {
  install.packages("needs")
}
library(needs)
needs(xfun, tidyverse, remotes, devtools, mice, pastecs,svglite, HLMdiag, gtsummary, cardx, flextable, lme4, nlme, pwr,huxtable, broom.mixed, patchwork, sjPlot, ggcorrplot, lmerTest, MuMIn, mlmhelpr, car, readxl)


```

```{r functions}
#| include: false

#### Average two numbers if there is a hyphen####
handle_hyphen <- function(data, column_name) {
  data %>%
    mutate(
      {{column_name}} := ifelse(
        is.na(.[[column_name]]), 
        NA,  # If the value is NA, keep it as NA
        ifelse(
          grepl("-", .[[column_name]]), 
          sapply(strsplit(.[[column_name]], "-"), function(x) mean(as.numeric(x), na.rm = TRUE)), 
          ifelse(
            .[[column_name]] == "", NA,  # Handle empty strings explicitly
            as.character(.[[column_name]])  # Keep the rest as characters
          )
        )
      )
    )
}
#df <- handle_hyphen(df, "WeeklyKM_base") # example use

#### Group similar words in a character variable ####
# Define the function
replace_patterns <- function(data, column_name, patterns) {
  # Dynamically evaluate the column and apply the replacements
  data %>%
    mutate(
      !!column_name := case_when(
        # Loop through the patterns and replacements
        !!!map2(patterns, names(patterns), function(pattern, replacement) {
          # Create case_when conditions: if the pattern matches, replace it
          grepl(pattern, .[[column_name]], ignore.case = TRUE) ~ replacement
        }),
        # Add a fallback to keep original values if no pattern matches
        TRUE ~ .[[column_name]]
      )
    )
}

## Example usage
## Define the patterns and their replacements
#patterns <- c(  "Kraftsport" = "kraft",   "Laufen" = "lauf")

## Apply the function to the 'Sport' column
#df <- replace_patterns(df, "Sport", patterns)

# Now df will have the patterns replaced in the 'Sport' column


#### Create Correlation Table #####
generate_correlation_table <- function(df, display_names) {
  library(Hmisc)
  library(flextable)
  library(officer)
  # Compute correlation matrix
  correlation_matrix <- rcorr(as.matrix(df))
  correlation_matrix_r <- round(correlation_matrix$r, digits = 2)
  
  # Extract lower triangle of the correlation matrix
  lower_triangle <- correlation_matrix_r[lower.tri(correlation_matrix_r)]
  
  # Create a clean correlation matrix
  correlation_matrix_clean <- matrix(NA, nrow = ncol(correlation_matrix_r), ncol = ncol(correlation_matrix_r))
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- lower_triangle
  
  # Compute significance stars
  stars_matrix <- matrix("", nrow = ncol(correlation_matrix_clean), ncol = ncol(correlation_matrix_clean))
  stars_matrix[correlation_matrix$P < 0.01 & correlation_matrix$P > 0] <- "**"
  stars_matrix[correlation_matrix$P >= 0.01 & correlation_matrix$P < 0.05 & correlation_matrix$P > 0] <- "*"
  
  # Append stars to the lower triangle of the correlation matrix
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- paste(correlation_matrix_clean[lower.tri(correlation_matrix_clean)], stars_matrix[lower.tri(stars_matrix)], sep = "")
  # Compute mean and standard deviation of variables
  means <- colMeans(df, na.rm = T) %>% round(2)
  sds <- apply(df, 2, sd, na.rm = T) %>% round(2)# 2 stands for "colums" here
  
  # Create data frame
  correlation_df <- data.frame(Measure = display_names, Mean = means,SD = sds, correlation_matrix_clean)
  
  colnames(correlation_df)[4:ncol(correlation_df)] <- as.character(1:ncol(correlation_matrix_clean))
  
  # Create flextable
  flextable(correlation_df) %>%
    set_header_labels(
      Measure = "Measure", 
      Mean = "Mean", 
      SD = "SD"
    ) %>%
    add_header_row(
      values = c("", "Descriptive Statistics", "Correlations"), 
      colwidths = c(1, 2, ncol(correlation_matrix_clean))
    ) %>%
    flextable::align(align = "center", part = "all") %>%
    flextable::autofit() %>%
    flextable::bold(part = "header") %>%
    flextable::font(fontname = "Times New Roman", part = "all") %>%
    flextable::fontsize(size = 12, part = "all") %>%
    flextable::padding(padding.top = 3, padding.bottom = 3, part = "all") %>%
    flextable::border_remove() %>%
    flextable::hline_top(border = fp_border(width = 1.5), part = "header") %>%
    flextable::hline_bottom(border = fp_border(width = 1.5), part = "body") %>%
    flextable::hline(border = fp_border(width = 1), part = "header")
}

## without descriptive statistics next to correlations
generate_correlation_table2 <- function(df, display_names, method = c("pearson", "spearman")) {
  library(Hmisc)
  library(flextable)
  library(officer)
  
  method <- match.arg(method)  # Ensure the method is either "pearson" or "spearman"
  
  # Compute correlation matrix and p-values
  corr_result <- rcorr(as.matrix(df), type = method)
  r <- round(corr_result$r, 2)
  p <- corr_result$P
  
  # Initialize matrix for formatted output
  formatted_matrix <- matrix("", ncol = ncol(r), nrow = nrow(r))
  
  for (i in 1:nrow(r)) {
    for (j in 1:ncol(r)) {
      if (i > j) {
        stars <- if (is.na(p[i, j])) {
          ""
        } else if (p[i, j] < 0.01) {
          "**"
        } else if (p[i, j] < 0.05) {
          "*"
        } else {
          ""
        }
        formatted_matrix[i, j] <- paste0(r[i, j], stars)
      }
    }
  }
  
  # Turn into a data frame with display names
  formatted_df <- data.frame(Measure = display_names, formatted_matrix, stringsAsFactors = FALSE)
  colnames(formatted_df)[2:ncol(formatted_df)] <- as.character(1:(ncol(formatted_df) - 1))
  
  # Create flextable
  flextable(formatted_df) %>%
    set_header_labels(Measure = "Measure") %>%
    add_header_row(values = c("", "Correlations"), colwidths = c(1, ncol(r))) %>%
    flextable::align(align = "center", part = "all") %>%
    flextable::autofit() %>%
    flextable::bold(part = "header") %>%
    flextable::font(fontname = "Times New Roman", part = "all") %>%
    flextable::fontsize(size = 12, part = "all") %>%
    flextable::padding(padding.top = 3, padding.bottom = 3, part = "all") %>%
    flextable::border_remove() %>%
    flextable::hline_top(border = fp_border(width = 1.5), part = "header") %>%
    flextable::hline_bottom(border = fp_border(width = 1.5), part = "body") %>%
    flextable::hline(border = fp_border(width = 1), part = "header")
}


## Example usage
#correlation_names <- c("Age", "Gender","Weekly Kilometers")
#x<-generate_correlation_table(df[,c("Age","Gender","WeeklyKM_base")], correlation_names)
#### Generate mean values for values wit 

# Generate mean values of all variables that have a certain pattern
#df$mean_goals <- rowMeans(df[, grepl("goal", names(df),ignore.case = T)], na.rm = TRUE)
mean_by_pattern<-function(df,searchstring){
  new_var <- rowMeans (df[,grepl(searchstring, names (df), ignore.case = T)], na.rm = T)
  return(new_var)
}
#df$meannew<-mean_by_pattern(df,"goal") #example use

####Descriptives-Funktion: Berechnet Typische deskriptive Werte für alle Variablen eines gegebenen Datensatzes: ######
#Calculate mean, sd, range, min, max of all variables. 

mean_sd_median_min_max <- function(df) {
  result <- df %>%
    # Select only numeric columns
    select(where(is.numeric)) %>%
    # Summarise with the desired statistics
    summarise(across(everything(), 
                     list(mean = ~round(mean(., na.rm = TRUE), digits = 2), 
                          sd = ~round(sd(., na.rm = TRUE), digits = 2),
                          median = ~round(median(., na.rm = TRUE), digits = 2),
                          min = ~min(., na.rm = TRUE),
                          max = ~max(., na.rm = TRUE))))
  
  # Create named list
  result_list <- setNames(as.list(result), paste(names(result), sep = ""))
  
  return(result_list)
}

#### Return all variables that are not normally distribute in the dataset####



which_var_not_normal_p <- function(df) {
  # Step 1: Select numeric vars with non-zero variance
  numeric_vars <- df %>% 
    select(where(is.numeric)) %>%
    select(where(~ var(.x, na.rm = TRUE) > 0))  # NEU: nur mit Varianz > 0
  
  if (ncol(numeric_vars) == 0) return(data.frame())  # keine geeigneten Variablen

  # Step 2: Get Shapiro-Wilk p-values for all
  desc_stats <- stat.desc(numeric_vars, basic = FALSE, norm = TRUE)
  
  # Step 3: Check if p-values row exists
  if (!"normtest.p" %in% rownames(desc_stats)) return(data.frame())

  norm_pvals <- desc_stats["normtest.p", ]
  
  # Step 4: Identify variables with p < .05
  non_normal_vars <- names(norm_pvals)[norm_pvals < 0.05]
  
  # Step 5: Recompute and return p-values of non-normal variables only
  if (length(non_normal_vars) == 0) return(data.frame())  # nothing non-normal
  
  pvals_df <- stat.desc(numeric_vars[non_normal_vars], basic = FALSE, norm = TRUE) %>%
    as.data.frame() %>%
    .["normtest.p", , drop = FALSE]

  return(pvals_df)
}



##### Show Histograms of all variables #####
print_all_histograms <- function(df, bins_n=20) {
  df_long <- df %>%
    pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>% filter(!is.na(value))
  
  plot<- ggplot(df_long, aes(value)) +
    geom_histogram(aes(y = after_stat(density)), colour = "black", fill = "white", bins = bins_n) +
    labs(x = NULL, y = NULL) +
    scale_y_continuous(guide = "none") +
    facet_wrap(~variable, scales = "free") + # Create separate panels for each variable
    stat_function(fun = dnorm,
                  args = list(mean = mean(df_long$value, na.rm = TRUE),
                              sd = sd(df_long$value, na.rm = TRUE)),
                  colour = "black", linewidth = 1)
  
  print (plot)
}


#### Print violin Boxplots####
print_all_violin_boxplots <- function(df, group_col = NULL, dodge_width = 1, facet_nrow = 2, facet_ncol = NULL, point_jitter = 0.1, custom_labels = NULL) {
  # Ensure the required libraries are loaded
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  
  # Convert the data to a long format, keeping only numeric columns
  df_long <- df %>%
    pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
    filter(!is.na(value))
  
  # Preserve the original order of variables
  variable_order <- colnames(df)[sapply(df, is.numeric)]
  df_long <- df_long %>%
    mutate(variable = factor(variable, levels = variable_order))
  
  # Add group column to the long format if provided
  if (!is.null(group_col)) {
    df_long <- df_long %>%
      mutate(Group = as.factor(df[[group_col]]))
  } else {
    df_long$Group <- "1" # Default group if no grouping is provided
  }
  
  # Create a named vector for custom labels if provided
  if (!is.null(custom_labels)) {
    label_mapping <- custom_labels
  } else {
    label_mapping <- setNames(unique(df_long$variable), unique(df_long$variable)) # Default to current names
  }
  
  # Create the plot
  plot <- ggplot(df_long, aes(x = variable, y = value, fill = Group)) +
    # Violin plot
    geom_violin(aes(fill = Group), linewidth = 1, color = "black", 
                show.legend = FALSE, position = position_dodge(width = dodge_width)) +
    # Boxplot
    geom_boxplot(aes(fill = Group), outlier.size = 3, outlier.shape = 18, outlier.colour = "grey", 
                 width = 0.1, position = position_dodge(width = dodge_width), show.legend = FALSE) +
    # Raw data points with horizontal jitter
    geom_point(position = position_jitter(width = point_jitter, height = 0), 
               size = 1.5, alpha = 0.6, aes(color = Group), show.legend = FALSE) +
    # Summary mean points
    stat_summary(mapping = aes(color = Group), fun = mean, geom = "point", shape = 4, size = 3, 
                 position = position_dodge(width = dodge_width), show.legend = FALSE) +
    # Custom scales
    scale_color_manual(values = c("black", "black")) +
    scale_fill_manual(values = c("1" = "white", "2" = "grey"),
                      labels = c("1" = "Group 1", "2" = "Group 2"),
                      name = "Group") +
    # Theme settings
    theme_classic(base_size = 14, base_family = "sans") +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank()) +
    # Faceting with custom labels
    facet_wrap(~variable, scales = "free", as.table = TRUE, nrow = facet_nrow, ncol = facet_ncol,
               labeller = labeller(variable = label_mapping))
  
  # Print the plot
  print(plot)
}

##Generate a talbe with descriptives

get_descriptive_table <- function(df, language = "German") {
  library(dplyr)
  library(pastecs)
  
  # Compute statistics
  df_stat <- df %>% 
    stat.desc(basic = FALSE, norm = TRUE) %>% 
    t() %>% 
    as.data.frame() %>% 
    select(-var, -coef.var, -SE.mean, -kurt.2SE, -normtest.W, -skew.2SE)
  
  # Adjust normtest.p formatting
  df_stat <- df_stat %>%
    mutate(
      normtest.p = ifelse(
        normtest.p < 0.001,
        "< .001",
        as.character(round(normtest.p, 3))
      )
    )
  
  # Round all numeric values
  df_stat <- df_stat %>%
    mutate(across(where(is.numeric), ~ round(., 3)))
  
  # Add rownames as a variable
  df_stat <- df_stat %>%
    mutate(Variable = rownames(df_stat)) %>%
    select(Variable, everything())
  
  # Rename columns based on language
  if (language == "German") {
    df_stat <- df_stat %>% 
      rename(
        Median = median,
        Schiefe = skewness,
        Exzess = kurtosis,
        Mittelwert = mean,
        "95% KI" = CI.mean.0.95,
        "SD" = std.dev,
        "p-Wert " = normtest.p
      )
  } else if (language == "English") {
    df_stat <- df_stat %>% 
      rename(
        Median = median,
        Skewness = skewness,
        Kurtosis = kurtosis,
        Mean = mean,
        "95% CI" = CI.mean.0.95,
        "SD" = std.dev,
        "p-Value" = normtest.p
      )
  } else {
    stop("Unsupported language. Please choose either 'German' or 'English'.")
  }
  return(df_stat)}

# Format p values for markdown reporting
format_p <- function(p, digits = 3) {
  ifelse(p < 0.001, "< .001",
    ifelse(p < 0.01, "< .01",
      ifelse(p < 0.05, "< .05",
        paste0("= ", round(p, digits))
      )
    )
  )
}


```

```{r citations}
#| include: false
# 
 if (!requireNamespace("excelbib", quietly = TRUE)) { 
 xfun::install_github("Enno-W/excelbib") 
} 
library(excelbib)

# Create .bib file from the excel list
xlsx_to_bib("https://bit.ly/scemotions-references")
# Add references or cite: https://bit.ly/scemotions_edit_references
```

```{r data import and transformation}
#| include: false
### Study 1 ###

df_study1 <- readxl::read_excel("df_study1.xlsx")
# Remove repeatedly measured variables with suffixes _1 to _6
df_study1 <- df_study1 %>%
  select(-matches("_(1|2|3|4|5|6)$"))

# exclude if the row is empty
df_study1 <- df_study1 %>%
  filter(!if_all(everything(), is.na))

# cöount original participants
original_n<-nrow (df_study1)

# deal with hyphens in the km and h data
df_study1 <- handle_hyphen(df_study1, "WeeklyKM_base") # See the script "Functions.R" to examine the function
df_study1 <- df_study1 %>%
  mutate(
    WeeklyH_base = gsub(",", ".", WeeklyH_base)
  )
df_study1 <- handle_hyphen(df_study1, "WeeklyH_base")

#### Making everything numeric####
df_study1[df_study1==-99]<-NA
sum(is.na(df_study1))
df_study1 <- df_study1 %>%
  mutate(across(
    .cols = -c(Programme, Age, Aim, Notes, Gender, Sport),
    .fns = ~ as.numeric(.)
  ))
sum(is.na(df_study1))



### Remove participants who were not following a structured exercise program

df_study1<-df_study1 %>% filter(Programme == 1|is.na(Programme))
filtered_n <-nrow(df_study1)


# ### Remove Participants with too many NAs
# df_study1$missings_amount <- rowSums(is.na(df_study1))
# df_study1 %>% filter(missings_amount <42  )

# Creating a vector with the names of all character-variables
character_vars <- df_study1 %>%
  select(where(is.character)) %>%
  names()

### Counting how many data points are present in the numeric data
data_amount <-  df_study1 %>% select(-"Programme" ,-"Age" , -"Aim", -"Notes"   ,-   "Gender" ,  -"Sport", -ends_with("_ave"))%>%  # Select (where colums are numeric)
  summarise(across(everything(), ~ sum(!is.na(.)))) %>% sum() # Apply summary functions to columns to create a new table of summary statistics. Summary functions take vectors as input and return one value, ~sum of values that are not NA. !! The "." is a placeholder for all the things that are passed throught the function !is.na()

# Counting the amount of missing data
missings_amount <- df_study1 %>% select(-"Programme" ,-"Age"    ,-   "Gender" ,  -"Sport", -ends_with("_ave")) %>% summarise(across(everything(), ~ sum(is.na(.)))) %>% sum()


#Calculating the percentage of missing data
missings_percentage <-round((missings_amount / data_amount) * 100, 0)


####rename the gender ####
df_study1 <- df_study1 %>%
  mutate(Gender = dplyr::recode(Gender, "1" = "male", "2" = "female", "3" = "diverse"))

#### Counting by gender
n_male<-nrow(df_study1 %>% filter(Gender == "male"))
n_female<-nrow(df_study1 %>% filter(Gender == "female"))
n_diverse<-nrow(df_study1 %>% filter(Gender == "diverse"))


patterns <- c(  "Kraftsport" = "kraft",   "Laufen" = "lauf")
df_study1 <- replace_patterns(df_study1, "Sport", patterns)# See the script "Functions.R" to examine the function

# count how many participants were in the sport categories
n_running<-df_study1 %>% filter( Sport == "Laufen") %>% nrow()
n_weight<-df_study1 %>% filter( Sport == "Kraftsport") %>% nrow()
n_triathlon <-df_study1 %>% filter( Sport == "Triathlon") %>% nrow()
n_other_sports <- nrow(df_study1)-(n_running+n_weight+n_triathlon)

### Study 2 ###

## This dataset contains identical entries from df_study1, but data was manually chceked for plausibility and completion, only data suitable for longitudinal analysis was retained. 
df_study2 <- read_excel("df_study2.xlsx")

#### Deal with hyphens in weekly_KM_base ####
df_study2 <- handle_hyphen(df_study2, "WeeklyKM_base")# See the script "Functions.R" to examine the function
df_study2 <- df_study2 %>%
  mutate(
    WeeklyH_base = gsub(",", ".", WeeklyH_base)
  )
df_study2 <- handle_hyphen(df_study2, "WeeklyH_base") 

#### Making everything numeric####
df_study2[] <- sapply(df_study2, as.numeric)

sum(is.na(df_study2))
df_study2[df_study2==-99]<-NA
sum(is.na(df_study2))

original_data_amount <- df_study2 %>% select(- "ID", -"Programme" ,-"Age"    ,-   "Gender" ,  -"SportCode", -ends_with("_ave"))%>%  # Select (where colums are numeric)
  summarise(across(everything(), ~ sum(!is.na(.)))) %>% sum() # Apply summary functions to columns to create a new table of summary statistics. Summary functions take vectors as input and return one value, ~sum of values that are not NA. !! The "." is a placeholder for all the things that are passed throught the function !is.na()
original_data_missings_amount <- df_study2 %>% select(- "ID", -"Programme" ,-"Age"    ,-   "Gender" ,  -"SportCode", -ends_with("_ave")) %>% summarise(across(everything(), ~ sum(is.na(.)))) %>% sum()
missings_percentage <-round((original_data_missings_amount / original_data_amount) * 100, 0)


####rename the gender ####
df_study2 <- df_study2 %>%
  mutate(Gender = dplyr::recode(Gender, "1" = "male", "2" = "female", "3" = "diverse"))

### Speed

##KM or H that are 0 should not be possible, so they are set to "NA"
df_study2[, c("SessionKM_1", "SessionKM_2", "SessionKM_3", "SessionKM_4", "SessionKM_5", "SessionKM_6" )]
df_study2[, c("SessionH_1", "SessionH_2", "SessionH_3", "SessionH_4", "SessionH_5", "SessionH_6" )]
df_study2 <- df_study2 %>% rename(SessionKMaverage = SessionKM_ave)
df_study2 <- df_study2 %>% rename(SessionHaverage = SessionH_ave)
df_study2 <- df_study2 %>%
  mutate(across(starts_with("SessionKM_"), ~ ifelse(. == 0, NA, .))) %>%
  mutate(across(starts_with("SessionH_"), ~ ifelse(. == 0, NA, .)))


df_study2$Speed_1<-(df_study2$SessionKM_1/df_study2$SessionH_1)*60 %>% as.numeric()
df_study2$Speed_2<-(df_study2$SessionKM_2/df_study2$SessionH_2)*60 %>% as.numeric()
df_study2$Speed_3<-(df_study2$SessionKM_3/df_study2$SessionH_3)*60 %>% as.numeric()
df_study2$Speed_4<-(df_study2$SessionKM_4/df_study2$SessionH_4)*60 %>% as.numeric()
df_study2$Speed_5<-(df_study2$SessionKM_5/df_study2$SessionH_5)*60 %>% as.numeric()
df_study2$Speed_6<-(df_study2$SessionKM_6/df_study2$SessionH_6)*60 %>% as.numeric()

df_study2$Speed_1_class <- ifelse(df_study2$Speed_1 < 5, "Swimming",
                     ifelse(df_study2$Speed_1 <= 20, "Running", "Cycling"))

df_study2$Speed_2_class <- ifelse(df_study2$Speed_2 < 5, "Swimming",
                     ifelse(df_study2$Speed_2 <= 20, "Running", "Cycling"))

df_study2$Speed_3_class <- ifelse(df_study2$Speed_3 < 5, "Swimming",
                     ifelse(df_study2$Speed_3 <= 20, "Running", "Cycling"))

df_study2$Speed_4_class <- ifelse(df_study2$Speed_4 < 5, "Swimming",
                     ifelse(df_study2$Speed_4 <= 20, "Running", "Cycling"))

df_study2$Speed_5_class <- ifelse(df_study2$Speed_5 < 5, "Swimming",
                     ifelse(df_study2$Speed_5 <= 20, "Running", "Cycling"))

df_study2$Speed_6_class <- ifelse(df_study2$Speed_6 < 5, "Swimming",
                     ifelse(df_study2$Speed_6 <= 20, "Running", "Cycling"))




#### sport specific z-scaling ####

##For baseline (According to sport Code)
df_study2 <- df_study2 %>%
  group_by(SportCode) %>%
  mutate(WeeklyKM_base = as.numeric(scale(WeeklyKM_base))) %>%
  ungroup()

### For repeated measures ##

# Speed
for (i in 1:6) {
  speed_var <- paste0("Speed_", i)
  class_var <- paste0("Speed_", i, "_class")

  df_study2 <- df_study2 %>%
    group_by(.data[[class_var]]) %>%
    mutate(!!speed_var := as.numeric(scale(.data[[speed_var]]))) %>%
    ungroup()
}

# KM
for (i in 1:6) {
  km_var <- paste0("SessionKM_", i)
  class_var <- paste0("Speed_", i, "_class")
 
  df_study2 <- df_study2 %>%
    group_by(.data[[class_var]]) %>%
    mutate(!!km_var := as.numeric(scale(.data[[km_var]]))) %>%
    ungroup()
}

#####Convert Data to long format#####
repeated_measures1<-df_study2 %>% ungroup() %>% select( matches("_[1-6]$")) %>% names()# This is a regex, a regular expression to find a certain pattern. The Dollar sign is for "Ends with". Learn more here: https://github.com/ziishaned/learn-regex/blob/master/translations/README-de.md

long_df <- df_study2 %>%
  pivot_longer(
    cols = all_of(repeated_measures1), # 
    names_to = c(".value", "Time"),   # Split into a base name and the timepoint
    names_pattern = "(.*)_(\\d+)"     # Regex to split column names like "Goal_1"
  ) %>%
  mutate(
    Time = as.numeric(Time)           # Convert extracted timepoint to numeric
  )

### Dependent variables should lead by one session 
 
 long_df <- long_df %>%
  arrange(ID, Time) %>%
  group_by(ID) %>%
  mutate(
    SessionKM_lead1 = lead(SessionKM, order_by = Time),
    SessionH_lead1 = lead(SessionH, order_by = Time),
    SessionRPE_lead1 = lead(SessionRPE, order_by = Time)
  ) %>%
  ungroup()

#### Centering #####
# Grand mean centering
long_df[, c("Pride_centered", "Dynamics_centered", "Locus_centered", "Globality_centered", "Affiliation_centered", "Achievement_centered", "Power_centered")] <- scale(long_df[, c("Pride", "Dynamics", "Locus", "Globality", "Affiliation", "Achievement", "Power")], center = TRUE, scale = FALSE)

# Group mean centering
long_df <- long_df %>%
  group_by(ID) %>%
  mutate(across(c(Pride, PA), 
                ~ . - mean(., na.rm = TRUE), 
                .names = "{.col}_cm_centered")) %>%
  ungroup()

## Rank or Log transformations
long_df$SessionH_lead1_log <- log(long_df$SessionH_lead1)
long_df$SessionH_lead1_log[is.infinite(long_df$SessionH_lead1_log)] <- 0.000001

long_df$SessionH_lead1_sqrt<-sqrt(long_df$SessionH_lead1)
long_df$ID<-long_df$ID %>% as.factor()



```

```{r values}
#| include: false

#### Creating a list with all commonly used descriptive statistics + other descriptive values ###############################################
descriptives_list <- mean_sd_median_min_max(df_study1)
vars_not_normal<-which_var_not_normal_p(df_study2)
vars_not_normal_with_p_values<-which_var_not_normal_p(df_study1) %>% mutate(across(where(is.numeric), ~ ifelse(. < 0.001, "< .001", paste("= ", round(., 3)))))

#### Power analysis ####
pwr_result <- pwr.r.test(n = NULL,         
                     r = 0.5,           
                     sig.level = 0.05,  
                     power = 0.95,      
                     alternative = "greater") 


SessionKM_shapirotest_after_z<-shapiro.test(df_study2$WeeklyKM_base)

```

```{r tables study 1}
#| include: false

## Study 1

#### correlation table ####
pride_variables<-c("Pride_base", "Hubris_base")
base_training_and_affect_variables<-df_study1 %>% ungroup() %>% select(ends_with("_base"), -matches("Pride_base|Hubris_base")) %>% names() 
motive_variables <- c("Achievement", "Affiliation", "Power", df_study1 %>% ungroup() %>% select(starts_with("Gen_")) %>% names())
attrib_variables <- c("Locus", "Dynamics", "Controlability_self")# 
correlation_variables<-c( pride_variables,base_training_and_affect_variables,motive_variables,attrib_variables)
corr_table<-df_study1[,correlation_variables] %>% 
  generate_correlation_table2(method= "spearman",c(
    "1. Pride", 
    "2. Hubris", 
    "3. Session Training Distance", 
    "4. Session Training Hours", 
    "5. Session Training RPE", 
    "6. Positive Affect", 
    "7. Negative Affect", 
    "8. Achievement", 
    "9. Affiliation", 
    "10. Power", 
    "11. Hope for Success", 
    "12. Fear of Failure", 
    "13. Hope for Belonging",
    "14. Fear of Rejection",
    "15. Hope for Control",
    "16. Fear of Loss of Control",
    "17. Locus",
    "18. Dynamics",
    "19. Controllability"
  ))

#### Skewness, Kurtosis and min-max range table###############################################

df_stat<-get_descriptive_table(df_study1[, correlation_variables], language = "English")

df_stat$Variable <- c(
  "1. Pride", 
    "2. Hubris", 
    "3. Session Training Distance", 
    "4. Session Training Hours", 
    "5. Session Training RPE", 
    "6. Positive Affect", 
    "7. Negative Affect", 
    "8. Achievement", 
    "9. Affiliation", 
    "10. Power", 
    "11. Hope for Success", 
    "12. Fear of Failure", 
    "13. Hope for Belonging",
    "14. Fear of Rejection",
    "15. Hope for Control",
    "16. Fear of Loss of Control",
    "17. Locus",
    "18. Dynamics",
    "19. Controllability"
)

table_stat<-df_stat %>% flextable() %>% flextable::theme_apa() %>% autofit()
 

```

```{r graphs study 1}
#| include: false


custom_labels <- c(
    "Pride_base" = "1. Pride", 
    "Hubris_base" = "2. Hubris", 
    "WeeklyKM_base" = "3. Weekly Training Distance (KM)", 
    "WeeklyH_base" = "4. Weekly Training Hours", 
    "WeeklyRPE_base" = "5. Weekly Training RPE", 
    "PA_base" = "6. Positive Affect", 
    "NA_base" = "7. Negative Affect", 
    "Achievement" = "8. Achievement", 
    "Affiliation" = "9. Affiliation", 
    "Power" = "10. Power", 
    "Gen_Success_hope" = "11. Hope for Success", 
    "Gen_Failure_fear" = "12. Fear of Failure", 
    "Gen_Belonging_hope" = "13. Hope for Belonging", 
    "Gen_Rejection_fear" = "14. Fear of Rejection", 
    "Gen_Control_hope" = "15. Hope for Control", 
    "Gen_LossControl_fear" = "16. Fear of Loss of Control", 
    "Locus" = "17. Locus", 
    "Dynamics" = "18. Dynamics", 
    "Controlability_self" = "19. Self-Controllability"
)

#print_all_histograms(df, bins_n = 30)
#print_all_histograms(df[correlation_variables])

 violin_plots<-print_all_violin_boxplots(df_study1[,correlation_variables], facet_ncol = 3, facet_nrow = NULL, custom_labels = custom_labels)


# long_df_filtered <- na.omit(long_df[, c(
#   "SessionKM_lead1", "Pride_cm_centered", "Dynamics_centered",
#   "PA_cm_centered", "Locus_centered", "Globality_centered",
#   "Affiliation_centered", "Achievement_centered", "Power_centered",
#   "Time", "ID"
# )])
# long_df_filtered$fitted_km <- fitted(model_SessionKM_3)
# long_df_filtered$fitted_km <- fitted(model_SessionKM_3)


# hlm_plot<-ggplot(long_df_filtered, aes(x = Time, y = fitted_km, group = ID, color = as.factor(ID))) +
#   geom_line(show.legend = F)+
#   geom_point(aes(x= Time, y= SessionKM_log), show.legend = F)+
#   labs(x = "Time", y = "Fitted Goal", title = "Predicted KM by ID") +
#   theme_minimal()+
#   labs(
#     x = "Session No. ",
#     y = "Running Distance",
#     color = "ID"
#   )

# Compute Spearman correlation coefficients
correlation_coefficients <- cor(df_study1[, correlation_variables], use = "pairwise.complete.obs", method = "spearman")

# Compute p-values for the Spearman correlations
p.mat <- cor_pmat(df_study1[, correlation_variables], method = "spearman")

# Generate the heatmap
heatmap <- ggcorrplot(
  correlation_coefficients,
  method = "circle",
  type = NULL,
  tl.srt = 90,
  p.mat = p.mat,
  insig = "blank"
) +
  scale_x_discrete(labels = custom_labels) +  # Apply custom labels to x-axis
  scale_y_discrete(labels = custom_labels)    # Apply custom labels to y-axis

# Print the heatmap
print(heatmap)

```

# Note

In this document, the results are generated from R code directly. You can access the full code [here](https://github.com/Enno-W/Self-Conscious-Emotions-in-Sport-and-Exercise/blob/main/index.qmd)

# Hypotheses

Hypothesis 1: The implicit achievement motive correlates positively with authentic pride.
In contrast, hubristic pride is associated with social dominance, reflecting the power motive, which leads to our second hypothesis:
Hypothesis 2: The implicit power motive correlates positively with hubristic pride.
As authentic pride relates to pro-social behavior and hubristic pride is associated with more anti-social behavior, our third hypothesis is:
Hypothesis 3 is: The implicit affiliation motive correlates positively with authentic pride and negatively with hubristic pride.
Additionally, in our fourth hypothesis, we assume that implicit motive component cues highlight situations as favorable for either authentic or hubristic pride:
Hypothesis 4: Hope for success, hope for control, and hope for social belonging correlate positively with authentic pride; fear of failure, fear of loss of control, and fear of social rejection correlate positively with hubristic pride.
Second, we hypothesize that causal attribution styles, as explicit regulation processes, correlate with either authentic or hubristic pride. This leads to the following hypotheses:
Hypothesis 5: Individuals who tend to attribute their achievements, power, or social gains internally, variably, controllably, and specifically will report higher levels of authentic pride.
Hypothesis 6: Individuals who tend to attribute their achievements, power, or social gains internally, stably, uncontrollably, and globally will report higher levels of hubristic pride.
Third, based on existing literature, we hypothesize that authentic and hubristic pride are associated with physical activity parameters. This leads to the following hypotheses:
Hypothesis 7: Authentic pride and its associated regulation processes correlate with higher physical activity parameters.
Hypothesis 8: Hubristic pride and its associated regulation processes correlate with lower physical activity parameters.


# Study 1

## Participants

Participants were adults with specific goals (e.g., running a marathon, competing in a triathlon, aesthetics, bodybuilding) and/or pre-defined workout routines (e.g., running routine, weight training log). A sample size calculation using G\*Power29 for testing one-sided point biserial correlation with |ρ| = .5, α = .05, and 1 – β = .95 indicated a necessary sample size of N = 34. As an incentive, participants were offered feedback on their results and suggestions for improving their workout routine motivation. *N* = `r original_n` participants completed the survey. After excluding participants that did not follow a training routine (*N* = `r original_n - filtered_n`), the sample consisted of *N* = `r filtered_n` recreational athletes (mean age = `r descriptives_list$Age_mean %>% round(2)` --36.13 years, SD =  `r descriptives_list$Age_sd %>% round(0)` --14.17), including `r n_female` females, `r n_male` males, and `r n_diverse` identifying as diverse. Participants followed individual workout routines averaging `r descriptives_list$WeeklyH_base_mean %>% round(2)` --7.69 hours per week (SD = `r descriptives_list$WeeklyH_base_sd %>% round(2)` --4.29) with a self-rated intensity (perceived exertion) of `r descriptives_list$WeeklyRPE_base_mean %>% round(2)`--5.17 (SD = `r descriptives_list$WeeklyRPE_base_sd %>% round(2)`--1.47). The sports they engaged in included running (n = `r n_running`), triathlon (n = `r n_triathlon`), weight training (n = `r n_weight`), and other endurance sports (n = `r n_other_sports`). `r missings_percentage` % of data were missing. 


## Measures

(see article)

## Procedure

(see article)

## Analysis

<!-- Hier noch baseline vs. session klar trennen -->
First, implicit motives for achievement, affiliation, and power motives were z-standardized, motive components were gender balanced t-standardized and the entire data was checked for normal distribution. Applying the Shapiro-Wilk test, session training distance (*p* `r vars_not_normal_with_p_values$WeeklyKM_base`), session training duration (*p* `r vars_not_normal_with_p_values$WeeklyH_base`) and perceived rate of exhaustion (*p* `r vars_not_normal_with_p_values$WeeklyH_base`) as well as fear of rejection (*p* `r vars_not_normal_with_p_values$Rejection_fear`) were not normally distributed. We then calculated the speed from the time and distance values for each session, and found a great variability. The speed in a session ranged from `r min(c(descriptives_list$Speed_1_min, descriptives_list$Speed_2_min, descriptives_list$Speed_3_min, descriptives_list$Speed_4_min, descriptives_list$Speed_5_min, descriptives_list$Speed_6_min), na.rm = TRUE) %>% round(0)` to `r max(c(descriptives_list$Speed_1_max, descriptives_list$Speed_2_max, descriptives_list$Speed_3_max, descriptives_list$Speed_4_max, descriptives_list$Speed_5_max, descriptives_list$Speed_6_max), na.rm = TRUE) %>% round(0)` kilometers per hour. As a consequence, we separated the data into three categories that fit with common speed of swimming, running, and cycling. If the speed was less than five kilometers per hour, the session was regarded as "swimming", less than 20 kilometers per hour as "running", and more than 20 kilometers per hour as "cycling". We then z-standardised the session distance within these groups. After this, running distance was normally distributed (*p* = `r round(SessionKM_shapirotest_after_z$p.value,3)`) Second, descriptive statistics were calculated. Third, according to the small sample size and normally distributed variables, Spearman rank correlations were calculated. For the analysis, the programming language R [@rlanguage2024] was used.

## Results

```{r}
#| label: tbl-stat
#| tbl-cap: Descriptive statistics table
#| apa-note: The p-value results from Shapiro-Wilk-tests of normality. 
table_stat
```

```{r}
#| label: tbl-corrtable
#| tbl-cap: Correlation matrix of Spearman Rho (Ρ) coefficients 
#| apa-note: "* p < .05, ** p < .01, *** p < .001."
corr_table
```
@tbl-stat  presents descriptive statistics, including mean, standard deviation, skewness, and kurtosis. The sample represents medium authentic pride and medium positive affect, low hubristic pride and low negative affect. Additionally, the sample shows a slightly stronger implicit affiliation motive but regular motive components in all 3 implicit motives. Further, the sample applies more internal, variable, and less uncontrollable attribution styles.  @fig-violin visualises the distibution of these variables. 

Correlations are presented in @tbl-corrtable and shown in @fig-corrplot.

```{r}
#| label: fig-corrplot
#| fig-cap: Correlation Plot showing Spearman Rho coefficients between investigated variables.
#| apa-note:  Bullets depict significant correlations (p < .05) and show positive (darker blue tones) or negative (darker red tones) correlations. Spearman's rank correlation coefficient was used. 
#| fig-height: 10
#| fig-width: 10
heatmap
```

```{r}
#| label: fig-violin
#| fig-cap: Violin Plot with boxplots
#| apa-note: The "X" represents the mean. Outliers are shown as grey diamonds. 
#| fig-height: 15
#| fig-width: 10
violin_plots
```

# Study 2

```{r HLM Table for KM}
#| include: false


##### Null model and ICC ####

null_model_km<- lme(SessionKM_lead1 ~ 1, 
                      data=long_df, 
                      random= ~1|ID, 
                      method="ML", 
                      na.action = na.omit) # NAs are ignored

summary(null_model_km)
# The "1" stands for the "intercept"
#The formula means: Fixed effects: for "Goal", only the intercepts are estimated. Random effects: "The intercept varies between participants". 
icc_km<-performance::icc(null_model_km)
# 
# model_SessionKM_rank<-lmer(SessionKM_rank~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + Pride * Affiliation + 
# Pride * Achievement + Pride * Power + (1 | ID), data = long_df)


model_SessionKM_1 <- lme(
  fixed = SessionKM_lead1 ~ Pride_cm_centered * PA_cm_centered,
  random = ~ 1 | ID,
  data = long_df,
  na.action = na.omit,
  method = "REML"
)

model_SessionKM_2 <- update(model_SessionKM_1,
  fixed = . ~ . +
    Pride_cm_centered * Dynamics_centered +
    Pride_cm_centered * Locus_centered +
    Pride_cm_centered * Globality_centered
)

model_SessionKM_3 <- update(model_SessionKM_2,
  fixed = . ~ . +
    Pride_cm_centered * Affiliation_centered +
    Pride_cm_centered * Achievement_centered +
    Pride_cm_centered * Power_centered
)

#### Does a correlation matrix improfe model fit? 

nlme::Variogram(model_SessionKM_3) # This seems inverse to the expected pattern: The larger the distance between measurement points, the more similar. 

model_SessionKM_3_corr <- lme(
  fixed = SessionKM_lead1 ~ Pride_cm_centered * PA_cm_centered +
    Pride_cm_centered * Dynamics_centered +
    Pride_cm_centered * Locus_centered +
    Pride_cm_centered * Globality_centered +
    Pride_cm_centered * Affiliation_centered +
    Pride_cm_centered * Achievement_centered +
    Pride_cm_centered * Power_centered,
  random = ~ 1 | ID,
  correlation = nlme::corAR1(form = ~ Time | ID),
  data = long_df,
  na.action = na.omit,
  method = "REML"
)

corAR_test_km<-anova(model_SessionKM_3, model_SessionKM_3_corr) # The model with correlation matrix does fit significantly better, but using the model does not change parameter estimates


Rsquared_km<-r.squaredGLMM(model_SessionKM_3)


### Table Output #####
# Distance
hlmtable<-huxreg("Nullmodell" = null_model_km, 
                 "Model 1" = model_SessionKM_1, 
                 "Model 2" = model_SessionKM_2, 
                 "Model 3" = model_SessionKM_3,
                 "Model 4" = model_SessionKM_3_corr,
                 statistics = NULL, 
                 number_format = 2, 
                 bold_signif = 0.05, 
                 tidy_args =  list(effects = "fixed"), 
                 error_pos="right",
                coefs = c(
    "(Intercept)" = "(Intercept)",
    "Pride" = "Pride_cm_centered",
     "Positive Affect" = "PA_cm_centered",
    "Dynamics" = "Dynamics_centered",
    "Locus" = "Locus_centered",
    "Globality" = "Globality_centered",
    "Affiliation" = "Affiliation_centered",
    "Achievement" = "Achievement_centered",
    "Power" = "Power_centered",
    "Pride × Dynamics" = "Pride_cm_centered:Dynamics_centered",
    "Pride × Positive Affect" = "Pride_cm_centered:PA_cm_centered",
    "Pride × Locus" = "Pride_cm_centered:Locus_centered",
    "Pride × Globality" = "Pride_cm_centered:Globality_centered",
    "Pride × Affiliation" = "Pride_cm_centered:Affiliation_centered",
    "Pride × Achievement" = "Pride_cm_centered:Achievement_centered",
    "Pride × Power" = "Pride_cm_centered:Power_centered")
)

    # weitere Variablen nach Bedarf ergänzen
```


```{r SessionKM graphs and assumptions check}
#| include: false
##### Checking Assumptions http://www.regorz-statistik.de/inhalte/r_HLM_2.html ###############################################

# Extract Residuals
SessionKM_residuals <- hlm_resid(model_SessionKM_3, level=1, include.ls = T) 
########Normality Plots #####
# Normality Plot with least square residuals
ndist_plot_km_ls <- ggplot(data = SessionKM_residuals  , aes(.ls.resid)) +
  geom_histogram(aes(y = after_stat(density)), bins=30) +
  stat_function(fun = dnorm,
                args = list(mean = mean(SessionKM_residuals  $.ls.resid),
                            sd = sd(SessionKM_residuals  $.ls.resid)), linewidth=2) # The "groups" (measurement points) are small, thus some are "rank deficient" - the regular residuals provide a less biased estimate

# Normality Plot with residuals
ndist_plot_km<- ggplot(data = SessionKM_residuals, aes(.resid)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 30,
                 fill = "gray80",
                 color = "black") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(SessionKM_residuals$.resid),
      sd = sd(SessionKM_residuals$.resid)
    ),
    linewidth = 1,
    color = "black",
    linetype = "dashed"
  ) +
  labs(
    x = "Residuals",
    y = "Density"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

# QQ line
qqplot_km<-ggplot(SessionKM_residuals, aes(sample = .resid)) +
  stat_qq(shape = 21, color = "black", fill = "gray80", size = 2) +
  stat_qq_line(color = "black", linetype = "dashed") +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )



###### Tests of normality ###############################################

#### Test for the Attribution Model #####
ntest_shapiro_km <-shapiro.test(SessionKM_residuals $.resid)  
ntest_ks_km <-ks.test(SessionKM_residuals  $.resid, "pnorm", mean(SessionKM_residuals  $.resid), sd(SessionKM_residuals  $.resid), exact = T)

######Variance Incluence Factor ####
model_SessionKM_2_lmer <- lmer(
  SessionKM_lead1 ~ 
    Pride_cm_centered * PA_cm_centered +
    Pride_cm_centered * Dynamics_centered + 
    Pride_cm_centered * Locus_centered + 
    Pride_cm_centered * Globality_centered +
    (1 | ID),
  data = long_df,
  na.action = na.omit,
  REML = TRUE
)
vif(model_SessionKM_2_lmer) 


##### Testing for homoscedasticity (homogeneity of variance of residuals), and outliers in the residuals####

resid_plot_km<-ggplot(data = SessionKM_residuals, aes(x = .fitted, y = .resid)) +
  geom_point(shape = 21, color = "black", fill = "gray80", size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = NULL
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

#resid_plot_km+resid_plot_km_no_logtransform
# Outliers (All)
ggplot(data = SessionKM_residuals , aes(y= .resid)) + theme_gray() + geom_boxplot()

#Outliers per individual
outl_plot_km <- ggplot(SessionKM_residuals, aes(x = .resid, y = ID)) +
  geom_boxplot(outlier.shape = 21, outlier.fill = "gray80", outlier.color = "black", outlier.size = 2) +
  labs(
    y = "Individual No.",
    x = "Residuals"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )


# Linearity
# Select relevant predictors (adjust this list as needed)
facet_df <- long_df %>%
  select(
    SessionKM_lead1,
    Pride_cm_centered,
    PA_cm_centered,
    Dynamics_centered,
    Locus_centered,
    Globality_centered,
    Affiliation_centered,
    Achievement_centered,
    Power_centered
  ) %>%
  pivot_longer(
    cols = -SessionKM_lead1,
    names_to = "Prädiktor",
    values_to = "Wert"
  )

linearity_plot <- ggplot(facet_df, aes(x = Wert, y = SessionKM_lead1)) +
  geom_point(alpha = 0.4, color = "grey40") +
  geom_smooth(method = "loess", se = FALSE, color = "firebrick", linewidth = 1) +
  facet_wrap(~ Prädiktor, scales = "free_x") +
  labs(
    x = "Predictors (centered)",
    y = "Training Distance"
  ) +
  theme_minimal(base_size = 16)

model_SessionKM_3_lmer <- lmer(
  SessionKM_lead1 ~ 
    Pride_cm_centered * PA_cm_centered +
    Pride_cm_centered * Dynamics_centered +
    Pride_cm_centered * Locus_centered +
    Pride_cm_centered * Globality_centered +
    Pride_cm_centered * Affiliation_centered +
    Pride_cm_centered * Achievement_centered +
    Pride_cm_centered * Power_centered +
    (1 | ID),
  data = long_df,
  na.action = na.omit,
  REML = TRUE
)

vif_KM_min<-vif(model_SessionKM_3_lmer) %>% min() %>% round (2)

vif_KM_max <- vif(model_SessionKM_3_lmer) %>% max() %>% round (2)


```

```{r HLM Table and graphs for H (Duration)}
#| include: false


##### Null model and ICC ####
null_model_h<- lme(SessionH_lead1_log ~ 1, 
                      data=long_df, 
                      random= ~1|ID, 
                      method="ML", 
                      na.action = na.omit) # See the documentation for lme: ?lme --> other options: na.exclude, na.pass...#
summary(null_model_h)
# The "1" stands for the "intercept"
#The formula means: Fixed effects: for "Goal", only the intercepts are estimated. Random effects: "The intercept varies between participants". 
icc_h<-performance::icc(null_model_h)

model_SessionH_1 <- lme(
  fixed =   SessionH_lead1 ~   
    Pride_cm_centered* PA_cm_centered,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit, 
  
  method = "REML"
)

model_SessionH_2 <- lme(
  fixed = SessionH_lead1 ~ 
        Pride_cm_centered* PA_cm_centered + 
    Pride_cm_centered* Dynamics_centered + 
    Pride_cm_centered* Locus_centered + 
    Pride_cm_centered* Globality_centered ,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit, 
  
  method = "REML"
)

model_SessionH_3 <- lme(
  fixed = SessionH_lead1 ~ 
    Pride_cm_centered * PA_cm_centered+ 
    Pride_cm_centered * Dynamics_centered + 
    Pride_cm_centered * Locus_centered + 
    Pride_cm_centered * Globality_centered + 
    Pride_cm_centered * Affiliation_centered + 
    Pride_cm_centered * Achievement_centered + 
    Pride_cm_centered * Power_centered,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit, 
  
  method = "REML"
)


Rsquared_h<-r.squaredGLMM(model_SessionH_3)
### Does a correlation structure improve the model?`
nlme::Variogram(model_SessionH_3) # Not the expected structure. With increasing distance, the variability should increase.


model_SessionH_3_corr <- lme(
  fixed = SessionH_lead1 ~ 
   Pride_cm_centered* PA_cm_centered + 
   Pride_cm_centered* Dynamics_centered +
   Pride_cm_centered* Locus_centered + 
   Pride_cm_centered* Globality_centered + 
   Pride_cm_centered* Affiliation_centered + 
   Pride_cm_centered* Achievement_centered + 
   Pride_cm_centered* Power_centered,
  random = ~ 1 | ID,
    correlation = nlme::corAR1(form = ~ Time | ID),
  data = long_df, 
  na.action=na.omit,
    method = "REML"
)
corAR_test_h <- anova( model_SessionH_3, model_SessionH_3_corr) # No improvement gained by modelling for autocorrelation
### Table Output #####
hlmtable_h<-huxreg("Nullmodell" = null_model_h, 
                   "Model 1" = model_SessionH_1, 
                   "Model 2" = model_SessionH_2, 
                   "Model 3" = model_SessionH_3,
                   "Model 3 without correlation matrix" = model_SessionH_3,
                   statistics = NULL, number_format = 2, bold_signif = 0.05, tidy_args =  list(effects = "fixed"), error_pos="right",
                  coefs = c(
    "(Intercept)" = "(Intercept)",
    "Pride" = "Pride_cm_centered",
    "Positive Affect" = "PA_cm_centered",
    "Dynamics" = "Dynamics_centered",
    "Locus" = "Locus_centered",
    "Globality" = "Globality_centered",
    "Affiliation" = "Affiliation_centered",
    "Achievement" = "Achievement_centered",
    "Power" = "Power_centered",
    "Pride × Dynamics" = "Pride_cm_centered:Dynamics_centered",
    "Pride × Positive Affect" = "Pride_cm_centered:PA_cm_centered",
    "Pride × Locus" = "Pride_cm_centered:Locus_centered",
    "Pride × Globality" = "Pride_cm_centered:Globality_centered",
    "Pride × Affiliation" = "Pride_cm_centered:Affiliation_centered",
    "Pride × Achievement" = "Pride_cm_centered:Achievement_centered",
    "Pride × Power" = "Pride_cm_centered:Power_centered"
  ))


```

```{r SessionH residual graphs and assumptions check}
#| include: false
##### Checking Assumptions http://www.regorz-statistik.de/inhalte/r_HLM_2.html ###############################################

# Extract Residuals
SessionH_residuals <- hlm_resid(model_SessionH_3, level=1, include.ls = T) 
########Normality Plots #####
# Normality Plot with least square residuals
ndist_plot_h_ls <- ggplot(data = SessionH_residuals  , aes(.ls.resid)) +
  geom_histogram(aes(y = after_stat(density)), bins=30) +
  stat_function(fun = dnorm,
                args = list(mean = mean(SessionH_residuals  $.ls.resid),
                            sd = sd(SessionH_residuals  $.ls.resid)), linewidth=2) # The "groups" (measurement points) are small, thus some are "rank deficient" - the regular residuals provide a less biased estimate

# Normality Plot with residuals
ndist_plot_h<- ggplot(data = SessionH_residuals, aes(.resid)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 30,
                 fill = "gray80",
                 color = "black") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(SessionH_residuals$.resid),
      sd = sd(SessionH_residuals$.resid)
    ),
    linewidth = 1,
    color = "black",
    linetype = "dashed"
  ) +
  labs(
    x = "Residuals",
    y = "Density"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

#ndist_plot_h+ndist_plot_h_no_logtransform
# QQ line
qqplot_h<-ggplot(SessionH_residuals, aes(sample = .resid)) +
  stat_qq(shape = 21, color = "black", fill = "gray80", size = 2) +
  stat_qq_line(color = "black", linetype = "dashed") +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

# Linearity
# Select relevant predictors (adjust this list as needed)
facet_df <- long_df %>%
  select(
    SessionH_lead1,
    Pride_cm_centered,
    PA_cm_centered,
    Dynamics_centered,
    Locus_centered,
    Globality_centered,
    Affiliation_centered,
    Achievement_centered,
    Power_centered
  ) %>%
  pivot_longer(
    cols = -SessionH_lead1,
    names_to = "Prädiktor",
    values_to = "Wert"
  )

linearity_plot_h <- ggplot(facet_df, aes(x = Wert, y = SessionH_lead1)) +
  geom_point(alpha = 0.4, color = "grey40") +
  geom_smooth(method = "loess", se = FALSE, color = "firebrick", linewidth = 1) +
  facet_wrap(~ Prädiktor, scales = "free_x") +
  labs(
    x = "Predictors (centered)",
    y = "Training Distance"
  ) +
  theme_minimal(base_size = 16)

model_SessionH_3_lmer <- lmer(
  SessionH_lead1 ~ 
    Pride_cm_centered * PA_cm_centered +
    Pride_cm_centered * Dynamics_centered +
    Pride_cm_centered * Locus_centered +
    Pride_cm_centered * Globality_centered +
    Pride_cm_centered * Affiliation_centered +
    Pride_cm_centered * Achievement_centered +
    Pride_cm_centered * Power_centered +
    (1 | ID),
  data = long_df,
  na.action = na.omit,
  REML = TRUE
)

vif_H_min<-vif(model_SessionH_3_lmer) %>% min() %>% round (2)

vif_H_max <- vif(model_SessionH_3_lmer) %>% max() %>% round (2)



###### Tests of normality ###############################################

#### Test for the Attribution Model #####
ntest_shapiro_h <-shapiro.test(SessionH_residuals $.resid)  
ntest_ks_h <-ks.test(SessionH_residuals  $.resid, "pnorm", mean(SessionH_residuals  $.resid), sd(SessionH_residuals  $.resid), exact = T)

##### Testing for homoscedasticity (homogeneity of variance of residuals), and outliers in the residuals####

resid_plot_h<-ggplot(data = SessionH_residuals, aes(x = .fitted, y = .resid)) +
  geom_point(shape = 21, color = "black", fill = "gray80", size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = NULL
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

# Outliers (All)
ggplot(data = SessionH_residuals , aes(y= .resid)) + theme_gray() + geom_boxplot()

#Outliers per individual
outl_plot_h <- ggplot(SessionH_residuals, aes(x = .resid, y = ID)) +
  geom_boxplot(outlier.shape = 21, outlier.fill = "gray80", outlier.color = "black", outlier.size = 2) +
  labs(
    y = "Individual No.",
    x = "Residuals"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )


```

```{r HLM Table for RPE}
#| include: false


##### Null model and ICC ####

null_model_rpe<- lme(SessionRPE_lead1 ~ 1, 
                      data=long_df, 
                      random= ~1|ID, 
                      method="ML", 
                      na.action = na.omit) # See the documentation for lme: ?lme --> other options: na.exclude, na.pass...#
# The "1" stands for the "intercept"
#The formula means: Fixed effects: for "Goal", only the intercepts are estimated. Random effects: "The intercept varies between participants". 
icc_rpe<-performance::icc(null_model_rpe)
# 
# model_SessionRPE<-lmer(SessionRPE~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + Pride * Affiliation + 
# Pride * Achievement + Pride * Power + (1 | ID), data = long_df)


model_SessionRPE_1 <- lme(
  fixed =   SessionRPE_lead1~   
    Pride_cm_centered* PA_cm_centered,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit, 
  method = "REML"
)

model_SessionRPE_2 <- lme(
  fixed = SessionRPE_lead1 ~ 
      Pride_cm_centered * PA_cm_centered+ 
    Pride_cm_centered* Dynamics_centered + 
    Pride_cm_centered* Locus_centered + 
    Pride_cm_centered* Globality_centered ,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit, 
  
  method = "REML"
)

model_SessionRPE_3 <- lme(
  fixed = SessionRPE_lead1 ~ 
    Pride_cm_centered * PA_cm_centered+ 
    Pride_cm_centered * Dynamics_centered + 
    Pride_cm_centered * Locus_centered + 
    Pride_cm_centered * Globality_centered + 
    Pride_cm_centered * Affiliation_centered + 
    Pride_cm_centered * Achievement_centered + 
    Pride_cm_centered * Power_centered,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit, 
  
  method = "REML"
)
## Does modelling autocorrelation improve the model? 
nlme::Variogram(model_SessionRPE_3) # Also does not support autocorrelation like it is theoretically expected (growing more dissimilar with increasing distance)

model_SessionRPE_4 <- lme(
  fixed = SessionRPE_lead1 ~ 
    Pride_cm_centered* PA_cm_centered+ 
    Pride_cm_centered* Dynamics_centered +
    Pride_cm_centered* Locus_centered + 
    Pride_cm_centered* Globality_centered + 
    Pride_cm_centered* Affiliation_centered + 
    Pride_cm_centered* Achievement_centered + 
    Pride_cm_centered* Power_centered,
  random = ~ 1 | ID,
   correlation = nlme::corAR1(form = ~ Time | ID),
  data = long_df, 
  na.action=na.omit,
    method = "REML"
)

## correlation matrix does not improve the model: 
intercorr_test_rpe<-anova(model_SessionRPE_4, model_SessionRPE_3 ) # even with different correlation structures, no improvement is gained.    

### Table Output #####
hlmtable_rpe<-huxreg("Nullmodell" = null_model_rpe, 
                     "Model 1" = model_SessionRPE_1, 
                     "Model 2" = model_SessionRPE_2, 
                     "Model 3" = model_SessionRPE_3,
                     "Model 3 without correlation matrix" = model_SessionRPE_4,
                     statistics = NULL, number_format = 2, bold_signif = 0.05, tidy_args =  list(effects = "fixed"), error_pos="right",
                     coefs = c(
    "(Intercept)" = "(Intercept)",
    "Pride" = "Pride_cm_centered",
    "Positive Affect" = "PA_cm_centered",
    "Dynamics" = "Dynamics_centered",
    "Locus" = "Locus_centered",
    "Globality" = "Globality_centered",
    "Affiliation" = "Affiliation_centered",
    "Achievement" = "Achievement_centered",
    "Power" = "Power_centered",
    "Pride × Dynamics" = "Pride_cm_centered:Dynamics_centered",
    "Pride × Positive Affect" = "Pride_cm_centered:PA_cm_centered",
    "Pride × Locus" = "Pride_cm_centered:Locus_centered",
    "Pride × Globality" = "Pride_cm_centered:Globality_centered",
    "Pride × Affiliation" = "Pride_cm_centered:Affiliation_centered",
    "Pride × Achievement" = "Pride_cm_centered:Achievement_centered",
    "Pride × Power" = "Pride_cm_centered:Power_centered"
  ))
# only use fixed effects in the parentheses

Rsquared_rpe<-r.squaredGLMM(model_SessionRPE_3)
```

```{r Variograms}

variogram_km  <-nlme::Variogram(model_SessionKM_3, 
                                form = ~ Time | ID)
dput(variogram_km)

variogram_h   <-nlme::Variogram(model_SessionH_3, 
                                form = ~ Time | ID)
dput(variogram_h)

variogram_rpe <-nlme::Variogram(model_SessionRPE_3, 
                                form = ~ Time | ID)
dput(variogram_rpe)

variogram_km <-structure(list(variog = c(1.15991898058474, 0.447036227324394, 
0.684989396378706, 0.217789011277918), dist = c(1, 2, 3, 4), 
    n.pairs = c(37L, 26L, 16L, 7L)), row.names = c(NA, 4L), class = c("Variogram", 
"data.frame"), collapse = TRUE)

variogram_h <- structure(list(variog = c(0.908464259718384, 1.13370321451414, 
0.420129132293349, 0.844873308138978), dist = c(1, 2, 3, 4), 
    n.pairs = c(53L, 37L, 23L, 10L)), row.names = c(NA, 4L), class = c("Variogram", 
"data.frame"), collapse = TRUE)

variogram_rpe<- structure(list(variog = c(0.836742906186201, 1.02148833359404, 
0.632955549381834, 0.420272035306193), dist = c(1, 2, 3, 4), 
    n.pairs = c(54L, 38L, 24L, 11L)), row.names = c(NA, 4L), class = c("Variogram", 
"data.frame"), collapse = TRUE)

ggplot(variogram_km, aes(x = dist, y = variog)) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    title = "Variogram: SessionKM",
    x = "Lag (distance between observations)",
    y = "Semivariance"
  ) +
  theme_minimal()

ggplot(variogram_h, aes(x = dist, y = variog)) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    title = "Variogram: SessionKM",
    x = "Lag (distance between observations)",
    y = "Semivariance"
  ) +
  theme_minimal()

ggplot(variogram_rpe, aes(x = dist, y = variog)) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    title = "Variogram: SessionKM",
    x = "Lag (distance between observations)",
    y = "Semivariance"
  ) +
  theme_minimal()
```


```{r SessionRPE residual graphs and assumptions check}
#| include: false
##### Checking Assumptions http://www.regorz-statistik.de/inhalte/r_HLM_2.html ###############################################

# Extract Residuals
SessionRPE_residuals <- hlm_resid(model_SessionRPE_3, level=1, include.ls = T) 
SessionRPE_residuals_no_logtransform <- hlm_resid(model_SessionRPE_4, level=1, include.ls = T) 
########Normality Plots #####
# Normality Plot with least square residuals
ndist_plot_rpe_ls <- ggplot(data = SessionRPE_residuals  , aes(.ls.resid)) +
  geom_histogram(aes(y = after_stat(density)), bins=30) +
  stat_function(fun = dnorm,
                args = list(mean = mean(SessionRPE_residuals  $.ls.resid),
                            sd = sd(SessionRPE_residuals  $.ls.resid)), linewidth=2) # The "groups" (measurement points) are small, thus some are "rank deficient" - the regular residuals provide a less biased estimate

# Normality Plot with residuals
ndist_plot_rpe<- ggplot(data = SessionRPE_residuals, aes(.resid)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 30,
                 fill = "gray80",
                 color = "black") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(SessionRPE_residuals$.resid),
      sd = sd(SessionRPE_residuals$.resid)
    ),
    linewidth = 1,
    color = "black",
    linetype = "dashed"
  ) +
  labs(
    x = "Residuals",
    y = "Density"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

#ndist_plot_rpe+ndist_plot_rpe_no_logtransform
# QQ line
qqplot_rpe<-ggplot(SessionRPE_residuals, aes(sample = .resid)) +
  stat_qq(shape = 21, color = "black", fill = "gray80", size = 2) +
  stat_qq_line(color = "black", linetype = "dashed") +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )
qqplot_rpe_no_logtransform <-ggplot(SessionRPE_residuals_no_logtransform, aes(sample = .resid)) +
  stat_qq(shape = 21, color = "black", fill = "black", size = 2) +
  stat_qq_line(color = "black", linetype = "dashed") +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )
# qqplot_rpe+qqplot_rpe_no_logtransform

# Linearity
# Select relevant predictors (adjust this list as needed)
facet_df <- long_df %>%
  select(
    SessionRPE_lead1,
    Pride_cm_centered,
    PA_cm_centered,
    Dynamics_centered,
    Locus_centered,
    Globality_centered,
    Affiliation_centered,
    Achievement_centered,
    Power_centered
  ) %>%
  pivot_longer(
    cols = -SessionRPE_lead1,
    names_to = "Prädiktor",
    values_to = "Wert"
  )

linearity_plot_rpe <- ggplot(facet_df, aes(x = Wert, y = SessionRPE_lead1)) +
  geom_point(alpha = 0.4, color = "grey40") +
  geom_smooth(method = "loess", se = FALSE, color = "firebrick", linewidth = 1) +
  facet_wrap(~ Prädiktor, scales = "free_x") +
  labs(
    x = "Predictors (centered)",
    y = "Training Distance"
  ) +
  theme_minimal(base_size = 16)

model_SessionRPE_3_lmer <- lmer(
  SessionRPE_lead1 ~ 
    Pride_cm_centered * PA_cm_centered +
    Pride_cm_centered * Dynamics_centered +
    Pride_cm_centered * Locus_centered +
    Pride_cm_centered * Globality_centered +
    Pride_cm_centered * Affiliation_centered +
    Pride_cm_centered * Achievement_centered +
    Pride_cm_centered * Power_centered +
    (1 | ID),
  data = long_df,
  na.action = na.omit,
  REML = TRUE
)



##### Variance Influence Factor ### 
model_SessionRPE_3_lmer <- lmer(
  SessionRPE_lead1 ~ 
    Pride_cm_centered * PA_cm_centered + 
    Pride_cm_centered * Dynamics_centered + 
    Pride_cm_centered * Locus_centered + 
    Pride_cm_centered * Globality_centered + 
    Pride_cm_centered * Affiliation_centered + 
    Pride_cm_centered * Achievement_centered + 
    Pride_cm_centered * Power_centered + 
    (1 | ID),
  data = long_df,
  na.action = na.omit,
  REML = TRUE
)

vif_RPE_min<-vif(model_SessionRPE_3_lmer) %>% min() %>% round (2)

vif_RPE_max <- vif(model_SessionRPE_3_lmer) %>% max() %>% round (2)

###### Tests of normality ###############################################

ntest_shapiro_rpe <-shapiro.test(SessionRPE_residuals $.resid)  
ntest_ks_rpe <-ks.test(SessionRPE_residuals  $.resid, "pnorm", mean(SessionRPE_residuals  $.resid), sd(SessionRPE_residuals  $.resid), exact = T)
ntest_ks_rpe_no_logtransform <-ks.test(SessionRPE_residuals_no_logtransform  $.resid, "pnorm", mean(SessionRPE_residuals_no_logtransform  $.resid), sd(SessionRPE_residuals_no_logtransform  $.resid), exact = T)


##### Visualisation of homoscedasticity (homogeneity of variance of residuals), and outliers in the residuals####

resid_plot_rpe<-ggplot(data = SessionRPE_residuals, aes(x = .fitted, y = .resid)) +
  geom_point(shape = 21, color = "black", fill = "gray80", size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = NULL
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )

resid_plot_rpe_no_logtransform<-ggplot(data = SessionRPE_residuals_no_logtransform, aes(x = .fitted, y = .resid)) +
  geom_point(shape = 15, color = "black", size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = NULL
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )
#resid_plot_rpe+resid_plot_rpe_no_logtransform
# Outliers (All)
ggplot(data = SessionRPE_residuals , aes(y= .resid)) + theme_gray() + geom_boxplot()

#Outliers per individual
outl_plot_rpe <- ggplot(SessionRPE_residuals, aes(x = .resid, y = ID)) +
  geom_boxplot(outlier.shape = 21, outlier.fill = "gray80", outlier.color = "black", outlier.size = 2) +
  labs(
    y = "Individual No.",
    x = "Residuals"
  ) +
  theme_classic(base_size = 12) +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.border = element_rect(fill = NA, color = "black")
  )


```


## Analysis

We excluded participants manually if they reported data on less than two training sessions, if they did not participate in a structured training programme, or if they were neither runners nor triathletes. Subsequent analyses were performed using the R language [@rlanguage2024]. 
 `r missings_percentage` of data were missings. Since most of the incomplete date was due to dropouts, we decided against multiple imputation, following the reccomendations of \[Lit\]. 
We assumed that the measurements at each point of measurement are intercorrelated. Thus, we used the nlme-package [@nlme] for the HLMs, since it facilitates modelling autocorrelation in longitudinal data. However, using the `variogram ()`- function, we examined the variograms of the HLMS and found no autocorrelation in the expected direction was present. Modelling the correlation structure did not result in different estimates for any model. Thus, we did not berücksichtigen autocorrelation in the models. 

Time was not included as predictor, since we did not expect a trend over time. The time-varying predictors pride and positive affect were group-mean centered, and the stable predictors attribution style (globality, dynamics, locus) and implicit motives (achievement, affiliation, power) were grand mean centered. This disentangles the between-person-effects from the within-person-effects, in line with recommendations of @Wang2015: The authors emphasize that a within-person effect can be present regardless of the between-person-effect, and vice versa. 
To illustrate this, it is possible that within a person, less pride after a run predicts a greater running distance in the next session. At the same time, people who are less proud on average do not necessarily run more.
Lastly, we z-standardised all predictors for easier interpretation. 

The results for the hierarchical linear models are presented separately for each the dependent variables of this study: Session distance, session duration and rate of perceived exhaustion. We used interaction terms of pride with positive affect, causal attribution and implicit motives to predict these dependent variables. The predictors were added in three steps: First, the interaction term of pride and positive affect, second, the interaction terms of pride and attribution styles, and third, interaction terms of pride and implicit motives. We then decided on the appropriate model based on fit indices and explanatory power, and then examined the model quality, checking for normal distribution of residuals, homoscedasticity, absence of multicollinearity, linearity and outliers. We also assessed if a correlation matrix improved the model fit.

## Participants

## Measures

(see article)

## Procedure

(see article)

## Results

 
## Session distance

In this model, we used interaction terms of pride with positive affect, causal attribution and implicit motives to predict running distance. The running distance was z-standardized to allow a comparison between the kilometer values reported by swimmers, runners, and triathlets. We added predictors to the null model in three steps: First, the interaction term of pride and positive affect, second, the interaction terms of pride and attribution styles, and third, interaction terms of pride and implicit motives. The result is shown in @tbl-hlmtable_km.

The intraclass correlation coefficient was *ICC* = `r icc_km[1]$ICC_adjusted %>% round (2)`. 
Model 3 included meaningful predictors and the lowest log-likelihood. Thus, Model 3 was selected for further analysis of statistical assumptions and the discussion. 

The residuals were normally distributed (*p* = `r round(ntest_ks_km $p.value, 2)` ). @fig-sessionkm_assumptions shows a QQ-plot and scatterplot of the residuals. Here, the residuals are concentrated in the left, indicating a slight deviation from the assumption of homoscedasticity. 

The variance influence factor ranged from *vif~min~* = `r vif_KM_min` to *vif~max~* = `r vif_KM_max`. 

Linearity was assessed by plotting each predictor against the outcome variable and examining LOESS-smoothed lines for deviations from linear trends. @fig-linearity shows substantial deviations from linearity. @fig-outliers shows the distribution of outliers in the residuals according to each individual. 

The marginal and conditional R-squared values, representing the portion of variance explained by the fixed effects and the random effects respectively, were *R^2^~marginal~* = `r Rsquared_km[1] %>% round(2)` and *R^2^~conditional~* = `r Rsquared_km[2] %>% round(2)`.





```{r}
#| label: tbl-hlmtable_km
#| tbl-cap: Hierarchical Linear Model Coefficients Predicting Running Distance
#| apa-note: Time varying predictors Pride and Positive Affect were cluster-mean-centered, fixed predictors were grand mean centered. In Model 4, the dependent variable was log-transformed to address heteroscedasticity and normality of residues. The fit indices of Model 4 are not directly comparable to models 1-3. 
hlmtable

```

```{r}
#| label: fig-sessionkm_assumptions
#| fig-cap: Histogram and QQ-Plot of Residuals for the running distance model
#| apa-note: This is the note below the figure.
#| fig-height: 10
#| fig-width: 10
(qqplot_km + resid_plot_km)
  
```

```{r}
#| label: fig-linearity
#| fig-cap: Predictors plottet against the target variable  with LOESS smoothing
#| apa-note: Pride and positive affect were cluster mean centered, the other predictors were grand mean centered. 
#| fig-height: 10
#| fig-width: 10
linearity_plot
```

```{r}
#| label: fig-outliers
#| fig-cap: Outliers of residuals for each individual
#| apa-note: Outliers are shown as grey circle
#| fig-height: 10
#| fig-width: 10
outl_plot_km
  
```


## Session duration

The parameters for this model that predicted session duration are shown in @tbl-hlmtable_h.
The ICC was *ICC* = `r icc_h$ICC_adjusted[1] %>% round(3)`. 

No model showed meaningful predictors of running distance. Model 3 was examined further.

The residuals were normally distributed in a Kolmogorov-Smirnov-Test (*p* = `r round(ntest_ks_h $p.value, 2)` ), but not in a Shapiro-Wilk-test (*p* = `r ntest_shapiro_h$p.value %>% format_p()`). A slight deviation from the normal distribution of residuals and the assumption of homoscedasticity is apparent in @fig-sessionh_assumptions. 

The variance influence factor ranged from *vif~min~* = `r vif_H_min` to *vif~max~* = `r vif_H_max`. 

@fig-linearity_h shows the predictors plotted against session duration to assess linearity. 

@fig-outliers_h shows two outliers in the residuals of the model. 

The marginal and conditional R-squared values were *R^2^~marginal~* = `r Rsquared_h[1] %>% round(2)` and *R^2^~conditional~* = `r Rsquared_h[2] %>% round(2)`.

```{r}
#| label: tbl-hlmtable_h
#| tbl-cap: Hierarchical Linear Model Coefficients Predicting Running Time
#| apa-note: Time varying predictors Pride and Positive Affect were cluster-mean-centered, fixed predictors were grand mean centered. In Model 4, the dependent variable was log-transformed to address heteroscedasticity and normality of residues. The fit indices of Model 4 are not directly comparable to models 1-3.  
hlmtable_h

```

```{r}
#| label: fig-sessionh_assumptions
#| fig-cap: Histogram of Residuals
#| apa-note: This is the note below the figure.
#| fig-height: 10
#| fig-width: 10
(qqplot_h + resid_plot_h)
  
```

```{r}
#| label: fig-outliers_h
#| fig-cap: Outliers of residuals for each individual
#| apa-note:  Outliers are shown as grey circle
#| fig-height: 10
#| fig-width: 10
outl_plot_h
  
```

```{r}
#| label: fig-linearity_h
#| fig-cap: Predictors plottet against the target variable  with LOESS smoothing
#| apa-note: This is the note below the figure.
#| fig-height: 10
#| fig-width: 10
linearity_plot_h
  
```
<!-- 1. Tabelle
2. ICC
Correlation matrix
3. Normalverteilung
4. homoscedasticity of residuals 8graph
5. multicollinearity vif
6. Linearität
7. Ausreißer
8. R squared
 -->

## Session Rate of Perceived Exhaustion

The regression coefficients are shown in @tbl-hlmtable_rpe. The third model with all predictors shows the lowest AIC and log likelihood and was tested for statistical assumptions and model quality.

A more stable attribution style predicted a lower RPE, and so did the interaction terms of pride with positive affect, pride with a more external locus of control, pride with a higher implicit affiliation as well as a higher power motive. Only the interaction between pride and the implicit achievement motive predicted a higher rate of perceived exhaustion for each session.

The residuals were normally distributed in a Kolmogorov-Smirnov test (*p* = `r round(ntest_ks_rpe $p.value, 2)` ). This test is less sensitivie to small deviations from the norm \[LIT\]. @fig-sessionrpe_assumptions shows the deviation from the normal distribution in a QQ-Plot along with the distribution of residuals. The linear patterns that emerge in the residuals are a result of the discrete values in the session RPE data.

The variance influence factor was between *vif~min~* = `r vif_RPE_min` and *vif~max~* = `r vif_RPE_max`.

The marginal and conditional R-squared values, representing the portion of variance explained by the fixed effects and the random effects respectively, were *R^2^~marginal~* = `r Rsquared_rpe[1] %>% round(2)` and *R^2^~conditional~* = `r Rsquared_rpe[2] %>% round(2)`.

 @fig-linearity_rpe shows the predictors plotted against the target variable. 

```{r}
#| label: tbl-hlmtable_rpe
#| tbl-cap: Hierarchical Linear Model Coefficients Predicting Session Rate of Perceived Exhaustion 
#| apa-note: Time varying predictors Pride and Positive Affect were cluster-mean-centered, fixed predictors were grand mean centered. In Model 4, the dependent variable was log-transformed to address heteroscedasticity and normality of residues. The fit indices of Model 4 are not directly comparable to models 1-3. 
hlmtable_rpe
```

```{r}
#| label: fig-sessionrpe_assumptions
#| fig-cap: Histogram of Residuals
#| apa-note: This is the note below the figure.
#| fig-height: 10
#| fig-width: 10
(qqplot_rpe + resid_plot_rpe)
  
```

```{r}
#| label: fig-linearity_rpe
#| fig-cap: Predictors plottet against the target variable  with LOESS smoothing
#| apa-note: This is the note below the figure.
#| fig-height: 10
#| fig-width: 10
linearity_plot
  
```

# Discussion

The variance influence factors suggest low multicollinearity [for a discussion see @Obrien2007].

## Limitations and Future Directions

## Conclusion

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
# 
:::
