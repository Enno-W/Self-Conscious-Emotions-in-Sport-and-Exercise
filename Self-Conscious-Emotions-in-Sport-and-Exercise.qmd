---
title: "My Title"
shorttitle: "Short Title in Running Header"
author:
  - name: Jane Doe
    corresponding: true
    orcid: 0000-0000-0000-0001
    email: janedoe@generic.edu
    affiliations:
      - name: Generic University
        department: Department of Scholarly Studies
        address: 1234 Capital St.
        city: New York
        region: NY
        country: USA
        postal-code: 12084-1234
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: ~
    related-report: ~
    conflict-of-interest: ~
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
link-citations: true
abstract: "This document is a template."
keywords: [keyword1, keyword2, keyword3]
bibliography: bibliography.bib
format:
  apaquarto-html: default
  apaquarto-docx: default
  apaquarto-pdf:
    documentmode: man
    
comments:
  hypothesis: true
---

```{r packages}
#| include: false

if (!requireNamespace("needs", quietly = TRUE)) {
  install.packages("needs")
}
library(needs)
needs(xfun, tidyverse, remotes, devtools, mice, pastecs,svglite, HLMdiag, gtsummary, cardx, flextable, lme4, nlme, pwr,huxtable, broom.mixed, patchwork, sjPlot, ggcorrplot, lmerTest, MuMIn)


```

```{r functions}
#| include: false

#### Average two numbers if there is a hyphen####
handle_hyphen <- function(data, column_name) {
  data %>%
    mutate(
      {{column_name}} := ifelse(
        is.na(.[[column_name]]), 
        NA,  # If the value is NA, keep it as NA
        ifelse(
          grepl("-", .[[column_name]]), 
          sapply(strsplit(.[[column_name]], "-"), function(x) mean(as.numeric(x), na.rm = TRUE)), 
          ifelse(
            .[[column_name]] == "", NA,  # Handle empty strings explicitly
            as.character(.[[column_name]])  # Keep the rest as characters
          )
        )
      )
    )
}
#df <- handle_hyphen(df, "WeeklyKM_base") # example use

#### Group similar words in a character variable ####
# Define the function
replace_patterns <- function(data, column_name, patterns) {
  # Dynamically evaluate the column and apply the replacements
  data %>%
    mutate(
      !!column_name := case_when(
        # Loop through the patterns and replacements
        !!!map2(patterns, names(patterns), function(pattern, replacement) {
          # Create case_when conditions: if the pattern matches, replace it
          grepl(pattern, .[[column_name]], ignore.case = TRUE) ~ replacement
        }),
        # Add a fallback to keep original values if no pattern matches
        TRUE ~ .[[column_name]]
      )
    )
}

## Example usage
## Define the patterns and their replacements
#patterns <- c(  "Kraftsport" = "kraft",   "Laufen" = "lauf")

## Apply the function to the 'Sport' column
#df <- replace_patterns(df, "Sport", patterns)

# Now df will have the patterns replaced in the 'Sport' column


#### Create Correlation Table #####
generate_correlation_table <- function(df, display_names) {
  library(Hmisc)
  library(flextable)
  library(officer)
  # Compute correlation matrix
  correlation_matrix <- rcorr(as.matrix(df))
  correlation_matrix_r <- round(correlation_matrix$r, digits = 2)
  
  # Extract lower triangle of the correlation matrix
  lower_triangle <- correlation_matrix_r[lower.tri(correlation_matrix_r)]
  
  # Create a clean correlation matrix
  correlation_matrix_clean <- matrix(NA, nrow = ncol(correlation_matrix_r), ncol = ncol(correlation_matrix_r))
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- lower_triangle
  
  # Compute significance stars
  stars_matrix <- matrix("", nrow = ncol(correlation_matrix_clean), ncol = ncol(correlation_matrix_clean))
  stars_matrix[correlation_matrix$P < 0.01 & correlation_matrix$P > 0] <- "**"
  stars_matrix[correlation_matrix$P >= 0.01 & correlation_matrix$P < 0.05 & correlation_matrix$P > 0] <- "*"
  
  # Append stars to the lower triangle of the correlation matrix
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- paste(correlation_matrix_clean[lower.tri(correlation_matrix_clean)], stars_matrix[lower.tri(stars_matrix)], sep = "")
  # Compute mean and standard deviation of variables
  means <- colMeans(df, na.rm = T) %>% round(2)
  sds <- apply(df, 2, sd, na.rm = T) %>% round(2)# 2 stands for "colums" here
  
  # Create data frame
  correlation_df <- data.frame(Measure = display_names, Mean = means,SD = sds, correlation_matrix_clean)
  
  colnames(correlation_df)[4:ncol(correlation_df)] <- as.character(1:ncol(correlation_matrix_clean))
  
  # Create flextable
  flextable(correlation_df) %>%
    set_header_labels(
      Measure = "Measure", 
      Mean = "Mean", 
      SD = "SD"
    ) %>%
    add_header_row(
      values = c("", "Descriptive Statistics", "Correlations"), 
      colwidths = c(1, 2, ncol(correlation_matrix_clean))
    ) %>%
    flextable::align(align = "center", part = "all") %>%
    flextable::autofit() %>%
    flextable::bold(part = "header") %>%
    flextable::font(fontname = "Times New Roman", part = "all") %>%
    flextable::fontsize(size = 12, part = "all") %>%
    flextable::padding(padding.top = 3, padding.bottom = 3, part = "all") %>%
    flextable::border_remove() %>%
    flextable::hline_top(border = fp_border(width = 1.5), part = "header") %>%
    flextable::hline_bottom(border = fp_border(width = 1.5), part = "body") %>%
    flextable::hline(border = fp_border(width = 1), part = "header")
}

## without descriptive statistics next to correlations
generate_correlation_table2 <- function(df, display_names) {
  library(Hmisc)
  library(flextable)
  library(officer)
  
  # Compute correlation matrix
  correlation_matrix <- rcorr(as.matrix(df))
  correlation_matrix_r <- round(correlation_matrix$r, digits = 2)
  
  # Extract lower triangle of the correlation matrix
  lower_triangle <- correlation_matrix_r[lower.tri(correlation_matrix_r)]
  
  # Create a clean correlation matrix
  correlation_matrix_clean <- matrix(NA, nrow = ncol(correlation_matrix_r), ncol = ncol(correlation_matrix_r))
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- lower_triangle
  
  # Compute significance stars
  stars_matrix <- matrix("", nrow = ncol(correlation_matrix_clean), ncol = ncol(correlation_matrix_clean))
  stars_matrix[correlation_matrix$P < 0.01 & correlation_matrix$P > 0] <- "**"
  stars_matrix[correlation_matrix$P >= 0.01 & correlation_matrix$P < 0.05 & correlation_matrix$P > 0] <- "*"
  
  # Append stars to the lower triangle of the correlation matrix
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- paste(correlation_matrix_clean[lower.tri(correlation_matrix_clean)], stars_matrix[lower.tri(stars_matrix)], sep = "")
  
  # Create data frame
  correlation_df <- data.frame(Measure = display_names, correlation_matrix_clean)
  
  colnames(correlation_df)[2:ncol(correlation_df)] <- as.character(1:ncol(correlation_matrix_clean))
  
  # Create flextable
  flextable(correlation_df) %>%
    set_header_labels(
      Measure = "Measure"
    ) %>%
    add_header_row(
      values = c("", "Correlations"), 
      colwidths = c(1, ncol(correlation_matrix_clean))
    ) %>%
    flextable::align(align = "center", part = "all") %>%
    flextable::autofit() %>%
    flextable::bold(part = "header") %>%
    flextable::font(fontname = "Times New Roman", part = "all") %>%
    flextable::fontsize(size = 12, part = "all") %>%
    flextable::padding(padding.top = 3, padding.bottom = 3, part = "all") %>%
    flextable::border_remove() %>%
    flextable::hline_top(border = fp_border(width = 1.5), part = "header") %>%
    flextable::hline_bottom(border = fp_border(width = 1.5), part = "body") %>%
    flextable::hline(border = fp_border(width = 1), part = "header")
}

## Example usage
#correlation_names <- c("Age", "Gender","Weekly Kilometers")
#x<-generate_correlation_table(df[,c("Age","Gender","WeeklyKM_base")], correlation_names)
#### Generate mean values for values wit 

# Generate mean values of all variables that have a certain pattern
#df$mean_goals <- rowMeans(df[, grepl("goal", names(df),ignore.case = T)], na.rm = TRUE)
mean_by_pattern<-function(df,searchstring){
  new_var <- rowMeans (df[,grepl(searchstring, names (df), ignore.case = T)], na.rm = T)
  return(new_var)
}
#df$meannew<-mean_by_pattern(df,"goal") #example use

####Descriptives-Funktion: Berechnet Typische deskriptive Werte für alle Variablen eines gegebenen Datensatzes: ######
#Calculate mean, sd, range, min, max of all variables. 
library(dplyr)

mean_sd_median_min_max <- function(df) {
  result <- df %>%
    # Select only numeric columns
    select(where(is.numeric)) %>%
    # Summarise with the desired statistics
    summarise(across(everything(), 
                     list(mean = ~round(mean(., na.rm = TRUE), digits = 2), 
                          sd = ~round(sd(., na.rm = TRUE), digits = 2),
                          median = ~round(median(., na.rm = TRUE), digits = 2),
                          min = ~min(., na.rm = TRUE),
                          max = ~max(., na.rm = TRUE))))
  
  # Create named list
  result_list <- setNames(as.list(result), paste(names(result), sep = ""))
  
  return(result_list)
}

#### Return all variables that are not normally distribute in the dataset####
which_var_not_normal<- function(df) {
  names<-df %>% select(where(is.numeric)) %>% select(where(~ !is.na(var(.)) && var(.) > 0)) %>% stat.desc (basic=F, norm=T) %>% as.data.frame() %>%.["normtest.p",] %>%   .[, . < 0.05 ] %>% names()
  return(names)
}

which_var_not_normal_p<- function(df) {
  names<-df %>% select(where(is.numeric)) %>% select(where(~ !is.na(var(.)) && var(.) > 0)) %>% stat.desc (basic=F, norm=T) %>% as.data.frame() %>%.["normtest.p",] %>%   .[, . < 0.05 ] %>% names()
  not_normal_data<-df[,names]  %>% stat.desc (basic=F, norm=T) %>% as.data.frame() %>%.["normtest.p",]
  return(not_normal_data)
}

##### Show Histograms of all variables #####
print_all_histograms <- function(df, bins_n=20) {
  df_long <- df %>%
    pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>% filter(!is.na(value))
  
  plot<- ggplot(df_long, aes(value)) +
    geom_histogram(aes(y = after_stat(density)), colour = "black", fill = "white", bins = bins_n) +
    labs(x = NULL, y = NULL) +
    scale_y_continuous(guide = "none") +
    facet_wrap(~variable, scales = "free") + # Create separate panels for each variable
    stat_function(fun = dnorm,
                  args = list(mean = mean(df_long$value, na.rm = TRUE),
                              sd = sd(df_long$value, na.rm = TRUE)),
                  colour = "black", linewidth = 1)
  
  print (plot)
}


#### Print violin Boxplots####
print_all_violin_boxplots <- function(df, group_col = NULL, dodge_width = 1, facet_nrow = 2, facet_ncol = NULL, point_jitter = 0.1, custom_labels = NULL) {
  # Ensure the required libraries are loaded
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  
  # Convert the data to a long format, keeping only numeric columns
  df_long <- df %>%
    pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
    filter(!is.na(value))
  
  # Preserve the original order of variables
  variable_order <- colnames(df)[sapply(df, is.numeric)]
  df_long <- df_long %>%
    mutate(variable = factor(variable, levels = variable_order))
  
  # Add group column to the long format if provided
  if (!is.null(group_col)) {
    df_long <- df_long %>%
      mutate(Group = as.factor(df[[group_col]]))
  } else {
    df_long$Group <- "1" # Default group if no grouping is provided
  }
  
  # Create a named vector for custom labels if provided
  if (!is.null(custom_labels)) {
    label_mapping <- custom_labels
  } else {
    label_mapping <- setNames(unique(df_long$variable), unique(df_long$variable)) # Default to current names
  }
  
  # Create the plot
  plot <- ggplot(df_long, aes(x = variable, y = value, fill = Group)) +
    # Violin plot
    geom_violin(aes(fill = Group), linewidth = 1, color = "black", 
                show.legend = FALSE, position = position_dodge(width = dodge_width)) +
    # Boxplot
    geom_boxplot(aes(fill = Group), outlier.size = 2, outlier.shape = 18, outlier.colour = "blue", 
                 width = 0.1, position = position_dodge(width = dodge_width), show.legend = FALSE) +
    # Raw data points with horizontal jitter
    geom_point(position = position_jitter(width = point_jitter, height = 0), 
               size = 1.5, alpha = 0.6, aes(color = Group), show.legend = FALSE) +
    # Summary mean points
    stat_summary(mapping = aes(color = Group), fun = mean, geom = "point", shape = 4, size = 3, 
                 position = position_dodge(width = dodge_width), show.legend = FALSE) +
    # Custom scales
    scale_color_manual(values = c("black", "black")) +
    scale_fill_manual(values = c("1" = "white", "2" = "grey"),
                      labels = c("1" = "Group 1", "2" = "Group 2"),
                      name = "Group") +
    # Theme settings
    theme_classic(base_size = 14, base_family = "sans") +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank()) +
    # Faceting with custom labels
    facet_wrap(~variable, scales = "free", as.table = TRUE, nrow = facet_nrow, ncol = facet_ncol,
               labeller = labeller(variable = label_mapping))
  
  # Print the plot
  print(plot)
}

##Generate a talbe with descriptives

get_descriptive_table <- function(df, language = "German") {
  library(dplyr)
  library(pastecs)
  
  # Compute statistics
  df_stat <- df %>% 
    stat.desc(basic = FALSE, norm = TRUE) %>% 
    t() %>% 
    as.data.frame() %>% 
    select(-var, -coef.var, -SE.mean, -kurt.2SE, -normtest.W, -skew.2SE)
  
  # Adjust normtest.p formatting
  df_stat <- df_stat %>%
    mutate(
      normtest.p = ifelse(
        normtest.p < 0.001,
        "< .001",
        as.character(round(normtest.p, 3))
      )
    )
  
  # Round all numeric values
  df_stat <- df_stat %>%
    mutate(across(where(is.numeric), ~ round(., 3)))
  
  # Add rownames as a variable
  df_stat <- df_stat %>%
    mutate(Variable = rownames(df_stat)) %>%
    select(Variable, everything())
  
  # Rename columns based on language
  if (language == "German") {
    df_stat <- df_stat %>% 
      rename(
        Median = median,
        Schiefe = skewness,
        Exzess = kurtosis,
        Mittelwert = mean,
        "95% KI" = CI.mean.0.95,
        "SD" = std.dev,
        "p-Wert " = normtest.p
      )
  } else if (language == "English") {
    df_stat <- df_stat %>% 
      rename(
        Median = median,
        Skewness = skewness,
        Kurtosis = kurtosis,
        Mean = mean,
        "95% CI" = CI.mean.0.95,
        "SD" = std.dev,
        "p-Value" = normtest.p
      )
  } else {
    stop("Unsupported language. Please choose either 'German' or 'English'.")
  }
  return(df_stat)}


```

```{r citations}
#| include: false
# 
 if (!requireNamespace("excelbib", quietly = TRUE)) { 
 xfun::install_github("Enno-W/excelbib") 
} 
library(excelbib)

# Create .bib file from the excel list
xlsx_to_bib("https://bit.ly/scemotions-references")
# Add references or cite: https://bit.ly/scemotions_edit_references
```

```{r data import and manipulation}
load("PRIMOCA_data_forR_20240203.Rdata")
df<-PRIMOCA_data_forR_20240203

#### Deal with hyphens in weekly_KM_base ####
df <- handle_hyphen(df, "WeeklyKM_base")# See the script "Functions.R" to examine the function
df <- df %>%
  mutate(
    WeeklyH_base = gsub(",", ".", WeeklyH_base)
  )
df <- handle_hyphen(df, "WeeklyH_base") # also a function from the function script, it averages two numbers out if the person wrote something like 40-45

####Dealing with weightlifter or other sport programs #### Hier klären wie genau verfahren werden soll!!

df$SportCode<-df$SportCode %>% recode(`3` = 1, `4` = 1)

#### Exclude participants#####
df<-df %>% filter(SportCode<=2)

#### Making everything numeric####
df[] <- sapply(df, as.numeric)

sum(is.na(df))
df[df==-99]<-NA

original_data_amount <-  df %>% select(- "ID", -"Programme" ,-"Age"    ,-   "Gender" ,  -"SportCode", -ends_with("_ave"))%>%  # Select (where colums are numeric)
  summarise(across(everything(), ~ sum(!is.na(.)))) %>% sum() # Apply summary functions to columns to create a new table of summary statistics. Summary functions take vectors as input and return one value, ~sum of values that are not NA. !! The "." is a placeholder for all the things that are passed throught the function !is.na()
original_data_missings_amount <- df %>% select(- "ID", -"Programme" ,-"Age"    ,-   "Gender" ,  -"SportCode", -ends_with("_ave")) %>% summarise(across(everything(), ~ sum(is.na(.)))) %>% sum()
missings_percentage <-round((original_data_missings_amount / original_data_amount) * 100, 0)

####rename the gender ####
df <- df %>%
  mutate(Gender = recode(Gender, "1" = "male", "2" = "female", "3" = "diverse"))


####z-Standardizing all variables####
 zstandard_varnames<-df %>% select(- "ID", -"Programme" ,-"Age"    ,-   "Gender" , - "WeeklyKM_base", "WeeklyH_base" , -"SportCode", -ends_with("_ave")) %>% names()
df <- df %>%  mutate_at(zstandard_varnames, ~ (scale(.))) # Hier werden dann [,1] im Variablen angezeigt, aber nur im Viewer???

  #sport specific centering ###Hier noch klären, was 3 und 4 bedeuten??
library(dplyr)
df %>% 
  group_by(SportCode) %>% 
  mutate(WeeklyKM_base=scale(WeeklyKM_base))

 
#####Convert Data to long format#####
weekly_measures<-select(df, matches("_[1-6]$")) %>% names()# This is a regex, a regular expression to find a certain pattern. The Dollar sign is for "Ends with". Learn more here: https://github.com/ziishaned/learn-regex/blob/master/translations/README-de.md

long_df <- df %>%
  pivot_longer(
    cols = all_of(weekly_measures), # 
    names_to = c(".value", "Time"),   # Split into a base name and the timepoint
    names_pattern = "(.*)_(\\d+)"     # Regex to split column names like "Goal_1"
  ) %>%
  mutate(
    Time = as.numeric(Time)           # Convert extracted timepoint to numeric
  )

#### Centering #####
# Grand mean centering
long_df[, c("Pride_centered", "Dynamics_centered", "PA_centered", "Locus_centered", "Globality_centered", "Affiliation_centered", "Achievement_centered", "Power_centered")] <- scale(long_df[, c("Pride", "Dynamics", "PA", "Locus", "Globality", "Affiliation", "Achievement", "Power")], center = TRUE, scale = FALSE)


```

```{r values and models}
#| include: false

#### Creating a list with all commonly used descriptive statistics + other descriptive values ###############################################
descriptives_list <- mean_sd_median_min_max(df)
vars_not_normal<-which_var_not_normal(df)
vars_not_normal_with_p_values<-which_var_not_normal_p(df) %>% mutate(across(where(is.numeric), ~ ifelse(. < 0.001, "< .001", as.character(round(.,3)))))



#### Power analysis ####
pwr_result <- pwr.r.test(n = NULL,         
                     r = 0.5,           
                     sig.level = 0.05,  
                     power = 0.95,      
                     alternative = "greater") 

#
#### Regression Analyses ###############################################

###ICC####
# Nullmodell
null_model_km<- lme(SessionKM ~ 1, 
                      data=long_df, 
                      random= ~1|ID, 
                      method="ML", 
                      na.action = na.omit) # See the documentation for lme: ?lme --> other options: na.exclude, na.pass...#
# The "1" stands for the "intercept"
#The formula means: Fixed effects: for "Goal", only the intercepts are estimated. Random effects: "The intercept varies between participants". 
summary(null_model_km)
icc_km<--performance::icc(null_model_km)

### Hierarchical Linear Models ####
long_df$ID<-long_df$ID %>% as.factor()

####with nlme{} ####
### km model with nlme{}
model_SessionKM <- lme(
  fixed = SessionKM ~ Pride * Dynamics + Pride * PA+ Pride * Locus + Pride * Globality + 
          Pride * Affiliation + Pride * Achievement + Pride * Power,
  random = ~ 1 | ID,
   correlation = corCAR1(form = ~ Time | ID),
  data = long_df, 
  na.action=na.omit
)

model_SessionKM_no_covar <- lme(
  fixed = SessionKM ~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + 
          Pride * Affiliation + Pride * Achievement + Pride * Power,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit
)

anova(model_SessionKM_no_covar,model_SessionKM)

## Duration model with nlme()
model_SessionH <- lme(
  fixed = SessionH ~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + 
          Pride * Affiliation + Pride * Achievement + Pride * Power,
  random = ~ 1 | ID,
   correlation = corCAR1(form = ~ Time | ID),
  data = long_df, 
  na.action=na.omit
)

model_SessionH_no_covar <- lme(
  fixed = SessionH ~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + 
          Pride * Affiliation + Pride * Achievement + Pride * Power,
  random = ~ 1 | ID,
  data = long_df, 
  na.action=na.omit
)

anova(model_SessionH_no_covar,model_SessionH)
# Since the models with an autocorrelation matrix fit slightly worse(higher AIC and BIC), there is no need to use the nlme-package. 

#### with lme4 #####
null_model_km <- lmer(SessionKM ~ 1 + (1 | ID), 
                      data = long_df, 
                      REML = FALSE,   # Maximum Likelihood (ML), not REML
                      na.action = na.omit)
summary(null_model_km)
model_SessionKM_lme4<- lmer(SessionKM ~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + Pride *
       Affiliation+ Pride * Achievement + Pride * Power + (1 | ID), data = long_df)
summary(model_SessionKM_lme4)
r.squaredGLMM(model_SessionKM)



model_SessionKM_lme4_grand_centered<- lmer(SessionKM ~ Pride_centered * Dynamics_centered + Pride_centered * PA_centered + Pride_centered * Locus_centered + Pride_centered * Globality_centered + Pride_centered *
       Affiliation + Pride_centered * Achievement + Pride_centered * Power + (1 | ID), data = long_df)
summary(model_SessionKM_lme4_grand_centered)
r.squaredGLMM(model_SessionKM)

# Group mean centering
long_df <- long_df %>%
  group_by(ID) %>%
  mutate(across(c(Pride, Dynamics, PA, Locus, Globality, 
                  Affiliation, Achievement, Power), 
                ~ . - mean(.), 
                .names = "{.col}_centered")) %>%
  ungroup()

model_SessionKM_lme4_group_centered<- lmer(SessionKM ~ Pride_centered * Dynamics_centered + Pride_centered * PA_centered + Pride_centered * Locus_centered + Pride_centered * Globality_centered + Pride_centered *
       Affiliation + Pride_centered * Achievement + Pride_centered * Power + (1 | ID), data = long_df)
summary(model_SessionKM_lme4_group_centered)
r.squaredGLMM(model_SessionKM)


# Duration model with lme4
model_SessionH_lme4 <- lmer(SessionH ~ Pride * Dynamics + Pride * PA + Pride * Locus + Pride * Globality + Pride * Affiliation +
Pride * Achievement + Pride * Power + (1 | ID), data = long_df)
summary(model_SessionH_lme4)
r.squaredGLMM(model_SessionH)


### Table Output #####
# Distance
hlmtable<-huxreg("Nullmodell" = null_model_km, "Raw Model" = model_SessionKM_lme4, "Grand Mean-Centered Model" =model_SessionKM_lme4_grand_centered, "Group Mean-Centered Model" = model_SessionKM_lme4_group_centered,  statistics = NULL, number_format = 3, bold_signif = 0.05, tidy_args =  list(effects = "fixed"), error_pos="right")# only use fixed effects in the parentheses


##### Checking Assumptions http://www.regorz-statistik.de/inhalte/r_HLM_2.html ###############################################
model_SessionKM_residuals  <- hlm_resid(model_SessionKM, level=1, include.ls = T) # Funktion aus HLMdiag-Package
#Now, I use the "..._residuals" to make a graph. these are the "Least squares residuals", and they have the advantage that influences from level 2 and 1 are not mixed up. 
ggplot(data = model_SessionKM_residuals  , aes(.ls.resid)) +
  geom_histogram(aes(y = after_stat(density)), bins=10) +
  stat_function(fun = dnorm,
                args = list(mean = mean(model_SessionKM_residuals  $.ls.resid),
                            sd = sd(model_SessionKM_residuals  $.ls.resid)), linewidth=2) 

###### Shapiro test of normality ###############################################

#### Test for the Attribution Model #####
test_normality_model_SessionKM_residuals <-shapiro.test(model_SessionKM_residuals $.ls.resid)  

### Markdown Output of p value ####
test_normality_model_SessionKM_residuals_p <-if (test_normality_model_SessionKM_residuals$p.value < 0.001) {
  "< .001"
} else {
  round( shaptest_goal_affect$p.value, 3)
}

##### Testing for homoscedasticity and Outliers: The variance of residuals must be constant for all values####
ggplot(data=model_SessionKM_residuals , aes(x=.ls.fitted, y=.ls.resid)) +
  geom_point() # Resembles a horizontal cone, speaking against homoscedasticity 
# Outliers
ggplot(data = model_SessionKM_residuals , aes(y= .ls.resid)) + theme_gray() + geom_boxplot() #overall
ggplot(data = model_SessionKM_residuals , aes( x= .ls.resid, y= as.factor(ID))) + theme_gray() + geom_boxplot() #

```

```{r tables}
#| include: false


#### correlation table ####
pride_variables<-c("Pride_base", "Hubris_base")
base_training_and_affect_variables<-df %>% select(ends_with("_base"), -matches("Pride_base|Hubris_base")) %>% names() 
motive_variables <- c("Achievement", "Affiliation", "Power", df %>% select(starts_with("Gen_")) %>% names())
attrib_variables <- c("Locus", "Dynamics", "Controlability_self")# 
correlation_variables<-c( pride_variables,base_training_and_affect_variables,motive_variables,attrib_variables)
corr_table<-df[,correlation_variables] %>% 
  generate_correlation_table2(c(
    "1. Pride", 
    "2. Hubris", 
    "3. Weekly Training Distance", 
    "4. Weekly Training Hours", 
    "5. Weekly Training RPE", 
    "6. Positive Affect", 
    "7. Negative Affect", 
    "8. Achievement", 
    "9. Affiliation", 
    "10. Power", 
    "11. Hope for Success", 
    "12. Fear of Failure", 
    "13. Hope for Belonging",
    "14. Fear of Rejection",
    "15. Hope for Control",
    "16. Fear of Loss of Control",
    "17. Locus",
    "18. Dynamics",
    "19. Controllability"
  ))

#### Skewness, Kurtosis and min-max range table###############################################

df_stat<-get_descriptive_table(df[, correlation_variables], language = "German")

df_stat$Variable <- c(
  "1. Pride", 
    "2. Hubris", 
    "3. Weekly Training Distance", 
    "4. Weekly Training Hours", 
    "5. Weekly Training RPE", 
    "6. Positive Affect", 
    "7. Negative Affect", 
    "8. Achievement", 
    "9. Affiliation", 
    "10. Power", 
    "11. Hope for Success", 
    "12. Fear of Failure", 
    "13. Hope for Belonging",
    "14. Fear of Rejection",
    "15. Hope for Control",
    "16. Fear of Loss of Control",
    "17. Locus",
    "18. Dynamics",
    "19. Controllability"
)

table_stat<-df_stat %>% flextable() %>% flextable::theme_apa() %>% autofit()
 

```


```{r graphs}
#| include: false


custom_labels <- c(
    "Pride_base" = "1. Pride", 
    "Hubris_base" = "2. Hubris", 
    "WeeklyKM_base" = "3. Weekly Training Distance (KM)", 
    "WeeklyH_base" = "4. Weekly Training Hours", 
    "WeeklyRPE_base" = "5. Weekly Training RPE", 
    "PA_base" = "6. Positive Affect", 
    "NA_base" = "7. Negative Affect", 
    "Achievement" = "8. Achievement", 
    "Affiliation" = "9. Affiliation", 
    "Power" = "10. Power", 
    "Gen_Success_hope" = "11. Hope for Success", 
    "Gen_Failure_fear" = "12. Fear of Failure", 
    "Gen_Belonging_hope" = "13. Hope for Belonging", 
    "Gen_Rejection_fear" = "14. Fear of Rejection", 
    "Gen_Control_hope" = "15. Hope for Control", 
    "Gen_LossControl_fear" = "16. Fear of Loss of Control", 
    "Locus" = "17. Locus", 
    "Dynamics" = "18. Dynamics", 
    "Controlability_self" = "19. Self-Controllability"
)

#print_all_histograms(df, bins_n = 30)
#print_all_histograms(df[correlation_variables])

 violin_plots<-print_all_violin_boxplots(df[correlation_variables], facet_ncol = 3, facet_nrow = NULL, custom_labels = custom_labels)

summary(model_SessionKM)
long_df_filtered <- na.omit(long_df[, c(all.vars(formula(model_SessionKM)),"Time", "ID")])
long_df_filtered$fitted_km <- fitted(model_SessionKM)
long_df_filtered$fitted_km <- fitted(model_SessionKM)


hlm_plot<-ggplot(long_df_filtered, aes(x = Time, y = fitted_km, group = ID, color = as.factor(ID))) +
  geom_line(show.legend = F)+
  geom_point(aes(x= Time, y= SessionKM), show.legend = F)+
  labs(x = "Time", y = "Fitted Goal", title = "Predicted KM by ID") +
  theme_minimal()+
  labs(
    x = "Session No. ",
    y = "Running Distance",
    color = "ID"
  )

time_plot<-plot_model(model_SessionKM, type = "pred", terms = "Time", show.data = T, jitter = .2, grid = T ,axis.title = c("Session No. ", "SessionKM"), title = "Running Distance")

Plot_across_time <- hlm_plot+time_plot

globality_plot<-plot_model(model_SessionKM, type = "pred", terms = c("Globality"), show.data = T, jitter = .2, grid = T, axis.title = c("Globality", "Running Distance in KM"), title = "Predicted running distance through Globality")# This shows more globality means less km, but its because of the negative interaction term with pride. 

# globality_plot<-plot_model(model_SessionKM, type = "pred", terms = c("Globality", "Pride"), show.data = T, jitter = .2, grid = T, axis.title = c("Globality", "Running Distance in KM"), title = "Predicted running distance through Globality") # Could refine with an interaction plot

# Diagnostic plots for model
Homoscedasticity_plotH <-plot_model(model_SessionH_no_covar, type = "diag") 


Homoscedasticity_plot <-plot_model(model_SessionKM, type = "resid") 

# Heatmap
p.mat <- cor_pmat(df[, correlation_variables])
str(df[, correlation_variables])
correlation_coefficients<-cor(df[, correlation_variables], use = "pairwise.complete.obs")

ggcorrplot(correlation_coefficients,
  p.mat = p.mat,
  type = NULL, insig = "blank", method= "circle", tl.srt = 90+
    scale_x_discrete(labels = custom_labels) +  # Apply custom labels to x-axis
  scale_y_discrete(labels = custom_labels)
)

heatmap<-ggcorrplot(correlation_coefficients, method= "circle", type =NULL,  tl.srt = 90,  p.mat = p.mat, insig = "blank")+
  scale_x_discrete(labels = custom_labels) +  # Apply custom labels to x-axis
  scale_y_discrete(labels = custom_labels)#https://www.youtube.com/watch?v=E3De2A73ako&t=1s
```

# Method

@kipchoge2020 @rstudio2024

## Analysis

-data cleaning 
We excluded participants manually if they reported data on less than two training sessions, if they did not participate in a structured training programme, or if they were neither runners nor triathletes. Because the variables had different scales, all variables were z-standardized. The baseline kilometer variable were standardized within their respective sport group, that is, running vs. triathlon. 
-imputation
- descriptive statistics
  - correlation

For the hierarchical linear models, all predictors were grand mean centered, as we were interested in the average influence of these variables across the six training units, and not the influence of individual fluctuations of in each training unit. 

## Participants

The participants were `r descriptives_list$Age_mean` old on average (*SD* = `r descriptives_list$Age_sd`) ranging from `r descriptives_list$Age_min` to `r descriptives_list$Age_max` years

## Measures

## Procedure

# Results

@fig-corrplot is a Heatmap

```{r}
#| label: fig-corrplot
#| fig-cap: Heatmap
#| apa-note: This is the note below the figure.
#| fig-height: 10
#| fig-width: 10
heatmap
```

```{r}
#| label: fig-violin
#| fig-cap: Violin Plot with boxplots
#| apa-note: This is the note below the figure.
#| fig-height: 15
#| fig-width: 10
violin_plots
```

```{r}
#| label: fig-time
#| fig-cap: Model results visualised
#| apa-note: This is the note below the figure.
#| fig-height: 15
#| fig-width: 10
Plot_across_time
```

```{r}
#| label: fig-globality_times_pride
#| fig-cap: Predictive Contribution of Globality, which interacts with pride
#| apa-note: This is the note below the figure.
#| fig-height: 15
#| fig-width: 10
globality_plot
```


```{r}
#| label: tbl-corrtable
#| tbl-cap: The Table Caption
#| apa-note: The note below the table.
corr_table

```

```{r}
#| label: tbl-hlmtable
#| tbl-cap: The Table Caption
#| apa-note: The note below the table.
hlmtable

```
# Discussion

## Limitations and Future Directions

## Conclusion

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

# Appendix

# Title for Appendix
