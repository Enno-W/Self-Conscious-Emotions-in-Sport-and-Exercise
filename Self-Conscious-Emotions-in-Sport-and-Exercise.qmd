---
title: "My Title"
shorttitle: "Short Title in Running Header"
author:
  - name: Jane Doe
    corresponding: true
    orcid: 0000-0000-0000-0001
    email: janedoe@generic.edu
    affiliations:
      - name: Generic University
        department: Department of Scholarly Studies
        address: 1234 Capital St.
        city: New York
        region: NY
        country: USA
        postal-code: 12084-1234
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: ~
    related-report: ~
    conflict-of-interest: ~
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
link-citations: true
abstract: "This document is a template."
keywords: [keyword1, keyword2, keyword3]
bibliography: bibliography.bib
format:
  apaquarto-html: default
  apaquarto-docx: default
  apaquarto-pdf:
    documentmode: man
    
    
---
```{r packages}
#| echo: false
#| eval: true 
#| output: false

if (!requireNamespace("needs", quietly = TRUE)) {
  install.packages("needs")
}
library(needs)
needs(xfun, tidyverse, remotes, devtools, mice)


```

```{r functions}
#| echo: false
#| eval: true 
#| output: false

#### Average two numbers if there is a hyphen####
handle_hyphen <- function(data, column_name) {
  data %>%
    mutate(
      {{column_name}} := ifelse(
        is.na(.[[column_name]]), 
        NA,  # If the value is NA, keep it as NA
        ifelse(
          grepl("-", .[[column_name]]), 
          sapply(strsplit(.[[column_name]], "-"), function(x) mean(as.numeric(x), na.rm = TRUE)), 
          ifelse(
            .[[column_name]] == "", NA,  # Handle empty strings explicitly
            as.character(.[[column_name]])  # Keep the rest as characters
          )
        )
      )
    )
}
#df <- handle_hyphen(df, "WeeklyKM_base") # example use

#### Group similar words in a character variable ####
# Define the function
replace_patterns <- function(data, column_name, patterns) {
  # Dynamically evaluate the column and apply the replacements
  data %>%
    mutate(
      !!column_name := case_when(
        # Loop through the patterns and replacements
        !!!map2(patterns, names(patterns), function(pattern, replacement) {
          # Create case_when conditions: if the pattern matches, replace it
          grepl(pattern, .[[column_name]], ignore.case = TRUE) ~ replacement
        }),
        # Add a fallback to keep original values if no pattern matches
        TRUE ~ .[[column_name]]
      )
    )
}

## Example usage
## Define the patterns and their replacements
#patterns <- c(  "Kraftsport" = "kraft",   "Laufen" = "lauf")

## Apply the function to the 'Sport' column
#df <- replace_patterns(df, "Sport", patterns)

# Now df will have the patterns replaced in the 'Sport' column


#### Create Correlation Table #####
generate_correlation_table <- function(df, display_names) {
  library(Hmisc)
  library(flextable)
  library(officer)
  # Compute correlation matrix
  correlation_matrix <- rcorr(as.matrix(df))
  correlation_matrix_r <- round(correlation_matrix$r, digits = 2)
  
  # Extract lower triangle of the correlation matrix
  lower_triangle <- correlation_matrix_r[lower.tri(correlation_matrix_r)]
  
  # Create a clean correlation matrix
  correlation_matrix_clean <- matrix(NA, nrow = ncol(correlation_matrix_r), ncol = ncol(correlation_matrix_r))
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- lower_triangle
  
  # Compute significance stars
  stars_matrix <- matrix("", nrow = ncol(correlation_matrix_clean), ncol = ncol(correlation_matrix_clean))
  stars_matrix[correlation_matrix$P < 0.01 & correlation_matrix$P > 0] <- "**"
  stars_matrix[correlation_matrix$P >= 0.01 & correlation_matrix$P < 0.05 & correlation_matrix$P > 0] <- "*"
  
  # Append stars to the lower triangle of the correlation matrix
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- paste(correlation_matrix_clean[lower.tri(correlation_matrix_clean)], stars_matrix[lower.tri(stars_matrix)], sep = "")
  # Compute mean and standard deviation of variables
  means <- colMeans(df, na.rm = T) %>% round(2)
  sds <- apply(df, 2, sd, na.rm = T) %>% round(2)# 2 stands for "colums" here
  
  # Create data frame
  correlation_df <- data.frame(Measure = display_names, Mean = means,SD = sds, correlation_matrix_clean)
  
  colnames(correlation_df)[4:ncol(correlation_df)] <- as.character(1:ncol(correlation_matrix_clean))
  
  # Create flextable
  flextable(correlation_df) %>%
    set_header_labels(
      Measure = "Measure", 
      Mean = "Mean", 
      SD = "SD"
    ) %>%
    add_header_row(
      values = c("", "Descriptive Statistics", "Correlations"), 
      colwidths = c(1, 2, ncol(correlation_matrix_clean))
    ) %>%
    flextable::align(align = "center", part = "all") %>%
    flextable::autofit() %>%
    flextable::bold(part = "header") %>%
    flextable::font(fontname = "Times New Roman", part = "all") %>%
    flextable::fontsize(size = 12, part = "all") %>%
    flextable::padding(padding.top = 3, padding.bottom = 3, part = "all") %>%
    flextable::border_remove() %>%
    flextable::hline_top(border = fp_border(width = 1.5), part = "header") %>%
    flextable::hline_bottom(border = fp_border(width = 1.5), part = "body") %>%
    flextable::hline(border = fp_border(width = 1), part = "header")
}

## without descriptive statistics next to correlations
generate_correlation_table2 <- function(df, display_names) {
  library(Hmisc)
  library(flextable)
  library(officer)
  
  # Compute correlation matrix
  correlation_matrix <- rcorr(as.matrix(df))
  correlation_matrix_r <- round(correlation_matrix$r, digits = 2)
  
  # Extract lower triangle of the correlation matrix
  lower_triangle <- correlation_matrix_r[lower.tri(correlation_matrix_r)]
  
  # Create a clean correlation matrix
  correlation_matrix_clean <- matrix(NA, nrow = ncol(correlation_matrix_r), ncol = ncol(correlation_matrix_r))
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- lower_triangle
  
  # Compute significance stars
  stars_matrix <- matrix("", nrow = ncol(correlation_matrix_clean), ncol = ncol(correlation_matrix_clean))
  stars_matrix[correlation_matrix$P < 0.01 & correlation_matrix$P > 0] <- "**"
  stars_matrix[correlation_matrix$P >= 0.01 & correlation_matrix$P < 0.05 & correlation_matrix$P > 0] <- "*"
  
  # Append stars to the lower triangle of the correlation matrix
  correlation_matrix_clean[lower.tri(correlation_matrix_clean)] <- paste(correlation_matrix_clean[lower.tri(correlation_matrix_clean)], stars_matrix[lower.tri(stars_matrix)], sep = "")
  
  # Create data frame
  correlation_df <- data.frame(Measure = display_names, correlation_matrix_clean)
  
  colnames(correlation_df)[2:ncol(correlation_df)] <- as.character(1:ncol(correlation_matrix_clean))
  
  # Create flextable
  flextable(correlation_df) %>%
    set_header_labels(
      Measure = "Measure"
    ) %>%
    add_header_row(
      values = c("", "Correlations"), 
      colwidths = c(1, ncol(correlation_matrix_clean))
    ) %>%
    flextable::align(align = "center", part = "all") %>%
    flextable::autofit() %>%
    flextable::bold(part = "header") %>%
    flextable::font(fontname = "Times New Roman", part = "all") %>%
    flextable::fontsize(size = 12, part = "all") %>%
    flextable::padding(padding.top = 3, padding.bottom = 3, part = "all") %>%
    flextable::border_remove() %>%
    flextable::hline_top(border = fp_border(width = 1.5), part = "header") %>%
    flextable::hline_bottom(border = fp_border(width = 1.5), part = "body") %>%
    flextable::hline(border = fp_border(width = 1), part = "header")
}

## Example usage
#correlation_names <- c("Age", "Gender","Weekly Kilometers")
#x<-generate_correlation_table(df[,c("Age","Gender","WeeklyKM_base")], correlation_names)
#### Generate mean values for values wit 

# Generate mean values of all variables that have a certain pattern
#df$mean_goals <- rowMeans(df[, grepl("goal", names(df),ignore.case = T)], na.rm = TRUE)
mean_by_pattern<-function(df,searchstring){
  new_var <- rowMeans (df[,grepl(searchstring, names (df), ignore.case = T)], na.rm = T)
  return(new_var)
}
#df$meannew<-mean_by_pattern(df,"goal") #example use

####Descriptives-Funktion: Berechnet Typische deskriptive Werte fÃ¼r alle Variablen eines gegebenen Datensatzes: ######
#Calculate mean, sd, range, min, max of all variables. 
library(dplyr)

mean_sd_median_min_max <- function(df) {
  result <- df %>%
    # Select only numeric columns
    select(where(is.numeric)) %>%
    # Summarise with the desired statistics
    summarise(across(everything(), 
                     list(mean = ~round(mean(., na.rm = TRUE), digits = 2), 
                          sd = ~round(sd(., na.rm = TRUE), digits = 2),
                          median = ~round(median(., na.rm = TRUE), digits = 2),
                          min = ~min(., na.rm = TRUE),
                          max = ~max(., na.rm = TRUE))))
  
  # Create named list
  result_list <- setNames(as.list(result), paste(names(result), sep = ""))
  
  return(result_list)
}

#### Return all variables that are not normally distribute in the dataset####
which_var_not_normal<- function(df) {
  names<-df %>% select(where(is.numeric)) %>% stat.desc (basic=F, norm=T) %>% as.data.frame() %>%.["normtest.p",] %>%   .[, . < 0.05 ] %>% names()
  return(names)
}

which_var_not_normal_p<- function(df) {
  names<-df %>% select(where(is.numeric)) %>% stat.desc (basic=F, norm=T) %>% as.data.frame() %>%.["normtest.p",] %>%   .[, . < 0.05 ] %>% names()
  not_normal_data<-df[,names]  %>% stat.desc (basic=F, norm=T) %>% as.data.frame() %>%.["normtest.p",]
  return(not_normal_data)
}

##### Show Histograms of all variables #####
print_all_histograms <- function(df, bins_n=20) {
  df_long <- df %>%
    pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>% filter(!is.na(value))
  
  plot<- ggplot(df_long, aes(value)) +
    geom_histogram(aes(y = after_stat(density)), colour = "black", fill = "white", bins = bins_n) +
    labs(x = NULL, y = NULL) +
    scale_y_continuous(guide = "none") +
    facet_wrap(~variable, scales = "free") + # Create separate panels for each variable
    stat_function(fun = dnorm,
                  args = list(mean = mean(df_long$value, na.rm = TRUE),
                              sd = sd(df_long$value, na.rm = TRUE)),
                  colour = "black", linewidth = 1)
  
  print (plot)
}


#### Print violin Boxplots####
print_all_violin_boxplots <- function(df, group_col = NULL, dodge_width = 1, facet_nrow = 2, facet_ncol = NULL, point_jitter = 0.1, custom_labels = NULL) {
  # Ensure the required libraries are loaded
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  
  # Convert the data to a long format, keeping only numeric columns
  df_long <- df %>%
    pivot_longer(cols = where(is.numeric), names_to = "variable", values_to = "value") %>%
    filter(!is.na(value))
  
  # Preserve the original order of variables
  variable_order <- colnames(df)[sapply(df, is.numeric)]
  df_long <- df_long %>%
    mutate(variable = factor(variable, levels = variable_order))
  
  # Add group column to the long format if provided
  if (!is.null(group_col)) {
    df_long <- df_long %>%
      mutate(Group = as.factor(df[[group_col]]))
  } else {
    df_long$Group <- "1" # Default group if no grouping is provided
  }
  
  # Create a named vector for custom labels if provided
  if (!is.null(custom_labels)) {
    label_mapping <- custom_labels
  } else {
    label_mapping <- setNames(unique(df_long$variable), unique(df_long$variable)) # Default to current names
  }
  
  # Create the plot
  plot <- ggplot(df_long, aes(x = variable, y = value, fill = Group)) +
    # Violin plot
    geom_violin(aes(fill = Group), linewidth = 1, color = "black", 
                show.legend = FALSE, position = position_dodge(width = dodge_width)) +
    # Boxplot
    geom_boxplot(aes(fill = Group), outlier.size = 2, outlier.shape = 18, outlier.colour = "blue", 
                 width = 0.1, position = position_dodge(width = dodge_width), show.legend = FALSE) +
    # Raw data points with horizontal jitter
    geom_point(position = position_jitter(width = point_jitter, height = 0), 
               size = 1.5, alpha = 0.6, aes(color = Group), show.legend = FALSE) +
    # Summary mean points
    stat_summary(mapping = aes(color = Group), fun = mean, geom = "point", shape = 4, size = 3, 
                 position = position_dodge(width = dodge_width), show.legend = FALSE) +
    # Custom scales
    scale_color_manual(values = c("black", "black")) +
    scale_fill_manual(values = c("1" = "white", "2" = "grey"),
                      labels = c("1" = "Group 1", "2" = "Group 2"),
                      name = "Group") +
    # Theme settings
    theme_classic(base_size = 14, base_family = "sans") +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank()) +
    # Faceting with custom labels
    facet_wrap(~variable, scales = "free", as.table = TRUE, nrow = facet_nrow, ncol = facet_ncol,
               labeller = labeller(variable = label_mapping))
  
  # Print the plot
  print(plot)
}

##Generate a talbe with descriptives

get_descriptive_table <- function(df, language = "German") {
  library(dplyr)
  library(pastecs)
  
  # Compute statistics
  df_stat <- df %>% 
    stat.desc(basic = FALSE, norm = TRUE) %>% 
    t() %>% 
    as.data.frame() %>% 
    select(-var, -coef.var, -SE.mean, -kurt.2SE, -normtest.W, -skew.2SE)
  
  # Adjust normtest.p formatting
  df_stat <- df_stat %>%
    mutate(
      normtest.p = ifelse(
        normtest.p < 0.001,
        "< .001",
        as.character(round(normtest.p, 3))
      )
    )
  
  # Round all numeric values
  df_stat <- df_stat %>%
    mutate(across(where(is.numeric), ~ round(., 3)))
  
  # Add rownames as a variable
  df_stat <- df_stat %>%
    mutate(Variable = rownames(df_stat)) %>%
    select(Variable, everything())
  
  # Rename columns based on language
  if (language == "German") {
    df_stat <- df_stat %>% 
      rename(
        Median = median,
        Schiefe = skewness,
        Exzess = kurtosis,
        Mittelwert = mean,
        "95% KI" = CI.mean.0.95,
        "SD" = std.dev,
        "p-Wert " = normtest.p
      )
  } else if (language == "English") {
    df_stat <- df_stat %>% 
      rename(
        Median = median,
        Skewness = skewness,
        Kurtosis = kurtosis,
        Mean = mean,
        "95% CI" = CI.mean.0.95,
        "SD" = std.dev,
        "p-Value" = normtest.p
      )
  } else {
    stop("Unsupported language. Please choose either 'German' or 'English'.")
  }
  return(df_stat)}


```

```{r citations}
#| echo: false
#| eval: true 
#| output: false
# 
 if (!requireNamespace("excelbib", quietly = TRUE)) { 
 xfun::install_github("Enno-W/excelbib") 
} 
library(excelbib)

# Create .bib file from the excel list
xlsx_to_bib("https://bit.ly/scemotions-references")
# Add references or cite: https://bit.ly/scemotions_edit_references
```

```{r data import and manipulation}
load(url("https://github.com/Enno-W/Self-Conscious-Emotions-in-Sport-and-Exercise/blob/main/241021PRIMOCA_data.Rdata?raw=true"))

sum(is.na(df))
#### counting excluded participants ####
raw_data_n <-nrow(df)

#### Removing Participants based on the filter varialbe "Programme"#####
df_filtered<-df %>% filter(Programme == 1|is.na(Programme))
filtered_n <-nrow(df_filtered)

#### NAs and Outliers ####
df_filtered$Missings_amount <- rowSums(is.na(df_filtered))
df_less_na<-df_filtered %>% filter(Missings_amount<50)#removing participants with more than 50 missings
na_removed_n <-nrow(df_less_na)
df<-df_less_na

#### Removing Variables that are not considered in this thesis ####
#df<- df %>% select(!matches("_fear|_hope|Achievement|Affiliation|Power|Programme|Pride|Hubris"))

#### Grouping the different kinds of sports and goals ####
df$Sport2 <- NA
df$Sport2[19] <- "Laufen" # Here, I add a second sport for participant 19
patterns <- c(  "Kraftsport" = "kraft",   "Laufen" = "lauf")
df <- replace_patterns(df, "Sport", patterns)# See the script "Functions.R" to examine the function

#### Deal with hyphenss in weekly_KM_base ####
df <- handle_hyphen(df, "WeeklyKM_base")# See the script "Functions.R" to examine the function
df <- df %>%
  mutate(
    WeeklyH_base = gsub(",", ".", WeeklyH_base)
  )
df <- handle_hyphen(df, "WeeklyH_base") # also a function from the function script, it averages two numbers out if the person wrote something like 40-45

#### Means based on a string that appears in the variable name####
df$Goal_ave <- mean_by_pattern(df, "goal")# see functions script
df$Commit_ave <- mean_by_pattern(df, "commit")
df$SessionKM_ave <- mean_by_pattern(df, "sessionkm")
df$SesseionH_ave <- mean_by_pattern(df, "sessionh")
df$SessionRPE_ave <- mean_by_pattern(df, "sessionrpe")
df$PA_ave <- mean_by_pattern(df, "pa_")
df$NA_ave <- mean_by_pattern(df,"na_")

####rename the gender ####
df <- df %>%
  mutate(Gender = recode(Gender, "1" = "MÃ¤nnlich", "2" = "Weiblich", "3" = "Divers"))

####change character varialbes to numeric####
df$WeeklyKM_base<- as.numeric(df$WeeklyKM_base)
df$WeeklyH_base<- as.numeric(df$WeeklyH_base)

#### Handling NAs####
df[df=="NaN"]<-NA

#### Variable to count training sessions ####
#function to return "No" if something is FALSE, and vice versa...
is.training.completed <- function(x) {
  if (is.na(x)) {
    return("No!")
  } else {
    return("Yes!")
  }
}
# Running that whole thing through a for loop
for (i in 1:6) {
  goal_col <- paste0("Goal_", i)
  newvar_col <- paste0("complete.", i)
  df[[newvar_col]] <- sapply(df[[goal_col]], FUN = is.training.completed)
}
# count the number of "Yes!"es in those
df$completed_count<- apply(select(df, starts_with(match = "complete.")), 1, function(x) length(which(x=="Yes!"))) # the "1" stands for rows here. see https://stackoverflow.com/questions/24015557/count-occurrences-of-value-in-a-set-of-variables-in-r-per-row

# ID Variable
df$ID<-1:nrow(df)

### Variable to count training sessions ####
#function to return "No" if something is FALSE, and vice versa...
is.training.completed <- function(x) {
  if (is.na(x)) {
    return("No!")
  } else {
    return("Yes!")
  }
}
# Running that whole thing through a for loop
for (i in 1:6) {
  goal_col <- paste0("Goal_", i)
  newvar_col <- paste0("complete_", i)
  df[[newvar_col]] <- sapply(df[[goal_col]], FUN = is.training.completed)
}
# count the number of "Yes!"es in those
df$completed_count<- apply(select(df, starts_with(match = "complete_")), 1, function(x) length(which(x=="Yes!"))) # the "1" stands for rows here. see https://stackoverflow.com/questions/24015557/count-occurrences-of-value-in-a-set-of-variables-in-r-per-row

# ####Multiple imputation####
# imp <- mice(df, m=5, maxit=5, method="pmm") # number of multiple imputations, maximum iterations, method: predictive mean matching
# # https://bookdown.org/mwheymans/bookmi/multiple-imputation.html#multiple-imputation-in-r
# df<-complete(imp)
```

```{r values and tables}
#| echo: false
#| eval: true 
#| output: false

#### Creating a list with all commonly used descriptive statistics + other descriptive values ###############################################
descriptives_list <- mean_sd_median_min_max(df)
vars_not_normal<-which_var_not_normal(df)
vars_not_normal_with_p_values<-which_var_not_normal_p(df) %>% mutate(across(where(is.numeric), ~ ifelse(. < 0.001, "< .001", as.character(round(.,3)))))
stat.desc(df$Locus, basic = F, norm = T)
stat.desc(df$Dynamics, basic = F, norm = T)
hist(df$Locus, breaks = 15)

####demographics table ###############################################
demographicsdata<- select(df, Age, Gender, Sport)  %>% tbl_summary( percent = "column", by = Gender) %>%  add_p() %>% add_overall() %>% as.data.frame()

data <- demographicsdata %>%
  rename_with(~ gsub("\\*\\*", "", .)) %>% # To remove the asterisks from the headers. 
  mutate(across(everything(), ~ ifelse(is.na(.), "", .)))

demographicstable<- data %>% flextable() %>% flextable::theme_apa() %>% autofit()

#### correlation table ####
pride_variables<-c("Pride_base", "Hubris_base")
base_training_and_affect_variables<-df %>% select(ends_with("_base"), -matches("Pride_base|Hubris_base")) %>% names() 
motive_variables <- c("Achievement", "Affiliation", "Power", df %>% select(starts_with("Gen_")) %>% names())
attrib_variables <- c("Locus", "Dynamics", "Controlability_self")# 
correlation_variables<-c( pride_variables,base_training_and_affect_variables,motive_variables,attrib_variables)
corr_table<-df[,correlation_variables] %>% 
  generate_correlation_table2(c(
    "1. Pride", 
    "2. Hubris", 
    "3. Weekly Training Distance", 
    "4. Weekly Training Hours", 
    "5. Weekly Training RPE", 
    "6. Positive Affect", 
    "7. Negative Affect", 
    "8. Achievement", 
    "9. Affiliation", 
    "10. Power", 
    "11. Hope for Success", 
    "12. Fear of Failure", 
    "13. Hope for Belonging",
    "14. Fear of Rejection",
    "15. Hope for Control",
    "16. Fear of Loss of Control",
    "17. Locus",
    "18. Dynamics",
    "19. Controllability"
  ))

#### Skewness, Kurtosis and min-max range table###############################################

df_stat<-get_descriptive_table(df[, correlation_variables], language = "German")

df_stat$Variable <- c(
  "Alter", 
  "Locus", 
  "VariabilitÃ¤t", 
  "n (Trainingseinheiten)", 
  "M (Ziellerreichung)", 
  "M (km pro Einheit)", 
  "M (h pro Einheit)", 
  "M (SessionRPE)", 
  "M (Negativer Affekt)", 
  "Baseline wÃ¶chentliche KM", 
  "Baseline wÃ¶chentliche H", 
  "Baseline wÃ¶chentliche RPE", 
  "Baseline negativer Affekt"
)

table_stat<-df_stat %>% flextable() %>% flextable::theme_apa() %>% autofit()
 


#### Power analysis ####
pwr_result <- pwr.r.test(n = NULL,         
                     r = 0.5,           
                     sig.level = 0.05,  
                     power = 0.95,      
                     alternative = "greater") 

#
#### Regression Analyses ###############################################

###ICC####
# Nullmodell
null_model_goal<- lme(Goal ~ 1, 
                      data=long_df, 
                      random= ~1|ID, 
                      method="ML", 
                      na.action = na.omit) # See the documentation for lme: ?lme --> other options: na.exclude, na.pass...#
# The "1" stands for the "intercept"
#The formula means: Fixed effects: for "Goal", only the intercepts are estimated. Random effects: "The intercept varies between participants". 
summary(null_model_goal)
icc_goal<--icc(null_model_goal) # The ICC is a lot lower with multiple imputation

###Centering###
#Center the time varying predictors to disentangle the repeated measurements from PA and NA traits
long_df$PositiveAffect_centered <- long_df$PositiveAffect - ave(long_df$PositiveAffect, long_df$ID, FUN = mean)
long_df$NegativeAffect_centered <- long_df$NegativeAffect - ave(long_df$NegativeAffect, long_df$ID, FUN = mean)

###General Linear Models####
#Hypothesis 1.1
h1.1_model <- glmer(completed_count ~ 1 + Time + Locus + Dynamics + (1 | ID), 
                    data = long_df, 
                    family = "poisson")
tidy(h1.1_model, effects = "fixed", conf.int = TRUE) # see huxreg documentation as for why this is necessary. This is defining a "tidy()-function"
summary(h1.1_model)

# Hypothesis 2.1
h2.1_model <- glmer(completed_count ~ 1 + Time + NA_base + NegativeAffect_centered + (1 | ID), 
                    data = long_df, 
                    family = "poisson")
tidy(h2.1_model, effects = "fixed", conf.int = TRUE)
summary(h2.1_model)

### Table Output ####
glmtable<-huxreg("Attributions-Modell (H 1.1)" = h1.1_model, "Affekt-Modell (H 2.1)" = h2.1_model ,statistics = NULL, number_format = 3, bold_signif = 0.05, omit_coefs = "Time" , error_pos = "right")

### Hierarchical Linear Model ####

#################Combined Advanced Models #################

#Without PA
goal_model_centered <- lme(
  Goal ~ 1 + Time + Locus + Dynamics +  NegativeAffect_centered,
  data = long_df,
  random = ~ Time| ID,
  method = "ML",
  na.action = na.omit,
  correlation = corAR1(form = ~ Time | ID)
)

goal_model_raw <- lme(
  Goal ~ 1 + Time + Locus + Dynamics +  NegativeAffect,
  data = long_df,
  random = ~ 1| ID,
  method = "ML",
  na.action = na.omit,
  correlation = corAR1(form = ~ Time | ID)
)

anova(goal_model_centered,goal_model_raw) # Both models are good,  choose this /(raw)) because it is slightly better and easier for interpretation 

goal_model_basic_pa <- lme(
  Goal ~ 1 + Time + Locus + Dynamics +  NegativeAffect+ PositiveAffect,
  data = long_df,
  random = ~ 1| ID,
  method = "ML",
  na.action = na.omit,
  correlation = corAR1(form = ~ Time | ID)
)
summary(goal_model_basic_pa)
anova(goal_model_raw, goal_model_basic_pa)
### Table Output #####
hlmtable<-huxreg("Nullmodell" = null_model_goal, "NA zentriert" = goal_model_centered, "NA nicht zentriert" = goal_model_raw , "Modell mit PA" = goal_model_basic_pa ,statistics = NULL, number_format = 3, bold_signif = 0.05, tidy_args =  list(effects = "fixed"), error_pos="right")# only use fixed effects in the parentheses


##### Checking Assumptions http://www.regorz-statistik.de/inhalte/r_HLM_2.html ###############################################
goal_model_pa_residuals  <- hlm_resid(goal_model_basic_pa, level=1) # Funktion aus HLMdiag-Package
#Now, I use the "..._residuals" to make a graph. these are the "Least squares residuals", and they have the advantage that influences from level 2 and 1 are not mixed up. 
ggplot(data = goal_model_pa_residuals , aes(.ls.resid)) +
  geom_histogram(aes(y = after_stat(density)), bins=10) +
  stat_function(fun = dnorm,
                args = list(mean = mean(goal_model_pa_residuals $.ls.resid),
                            sd = sd(goal_model_pa_residuals $.ls.resid)), linewidth=2) 

###### Shapiro test of normality ###############################################

#### Test for the Attribution Model #####
shaptest_goal_attrib <-shapiro.test(goal_model_pa_residuals $.ls.resid)

### Markdown Output of p value ####
shaptest_goal_attrib_p <-if (shaptest_goal_attrib$p.value < 0.001) {
  "< .001"
} else {
  round( shaptest_goal_affect$p.value, 3)
}

##### Testing for homoscedasticity and Outliers: The variance of residuals must be constant for all values####
ggplot(data=goal_model_pa_residuals , aes(x=.ls.fitted, y=.ls.resid)) +
  geom_point() # Weird linear patterns emerge: This means, homoscedasticity is violated. 
# Outliers
ggplot(data = goal_model_pa_residuals , aes(y= .ls.resid)) + theme_gray() + geom_boxplot() #overall
ggplot(data = goal_model_pa_residuals , aes( x= .ls.resid, y= as.factor(ID))) + theme_gray() + geom_boxplot() # per group, the outliers are pretty random and the width varies substantially between groups


# AusreiÃer
ggplot(data = goal_model_pa_residuals , aes(y= .resid)) + theme_gray() + geom_boxplot()
ggplot(data = goal_model_pa_residuals , aes(x= .resid, y= as.factor(ID))) + theme_gray() + geom_boxplot()
# Assumptions of normality and homoscedasticity are violated. Possible solutions could come from "robustlmm" package - to deal with "contamination", or using robust standard errors with "clubSandwich" and lme4


# AusreiÃer
ggplot(data = goal_model_pa_residuals , aes(y= .resid)) + theme_gray() + geom_boxplot()
ggplot(data = goal_model_pa_residuals , aes(x= .resid, y= as.factor(ID))) + theme_gray() + geom_boxplot()
# Assumptions of normality and homoscedasticity are violated. Possible solutions could come from "robustlmm" package - to deal with "contamination", or using robust standard errors with "clubSandwich" and lme4



```
# Method
@kipchoge2020
@rstudio2024

## Participants

## Measures

## Procedure

# Results

# Discussion

## Limitations and Future Directions

## Conclusion

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

# Appendix

# Title for Appendix
